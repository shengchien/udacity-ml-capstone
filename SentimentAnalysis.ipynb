{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Word2Vec and XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Requirements\n",
    "\n",
    "- python 3.6.5\n",
    "- numpy 1.14.2\n",
    "- panda 0.22.0\n",
    "- gensim 3.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \n",
      "[GCC 7.2.0]\n",
      "numpy: 1.14.2\n",
      "panda: 0.22.0\n",
      "gensim: 3.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "print(\"python: {}\".format(sys.version))\n",
    "print(\"numpy: {}\".format(np.version.version))\n",
    "print(\"panda: {}\".format(pd.__version__))\n",
    "print(\"gensim: {}\".format(gensim.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset used in this project can be found at <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\">Large Movie Review Dataset</a> \\[1\\]. For convenience, the files from the dataset have been concatenated into labeled_train.csv, unlabled_train.csv, and labeled_test.csv compressed in dataset.zip. Unzip dataset.zip before running this project.\n",
    "\n",
    "#### Reference\n",
    "\n",
    "\\[1\\] Andrew  L.  Maas,  Raymond  E.  Daly,  Peter  T.  Pham,  Dan  Huang,  An-drew Y. Ng, and Christopher Potts.  Learning word vectors for sentimentanalysis.   InProceedings of the 49th Annual Meeting of the Associationfor Computational Linguistics: Human Language Technologies, pages 142–150,  Portland,  Oregon,  USA,  June  2011.  Association  for  ComputationalLinguistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabeled train size: 50000\n",
      "Labeled train size: 25000\n",
      "Labeled test size: 25000\n",
      "Word2Vec train size: 75000\n"
     ]
    }
   ],
   "source": [
    "# Read train/test datasets\n",
    "from gensim.utils import simple_preprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Enable logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Ignore deprecation warning for concision\n",
    "# Please see https://github.com/scikit-learn/scikit-learn/issues/10449\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "\n",
    "# This method tokenizes each comment into lowercase token array.\n",
    "# gensim.utils.simple_preprocess ignores tokens that are too short or too long (min_len=2, max_len=15 by default).\n",
    "def preprocess(comments):\n",
    "    return np.array([simple_preprocess(c) for c in np.array(comments)])\n",
    "\n",
    "\n",
    "# Load training and test sets and apply the basic preprocessing. \n",
    "labeled_train = pd.read_csv(\"dataset/labeled_train.csv\", sep='\\t', header=None, names=['sentiment', 'comment'])\n",
    "X_train = preprocess(labeled_train['comment'])\n",
    "y_train = np.array(labeled_train['sentiment'])\n",
    "\n",
    "labeled_test = pd.read_csv(\"dataset/labeled_test.csv\", sep='\\t', header=None, names=['sentiment', 'comment'])\n",
    "X_test = preprocess(labeled_test['comment'])\n",
    "y_test = np.array(labeled_test['sentiment'])\n",
    "\n",
    "unlabeled_train = pd.read_csv(\"dataset/unlabeled_train.csv\", sep='\\t', header=None, names=['comment'])\n",
    "Z_train = preprocess(unlabeled_train['comment'])\n",
    "\n",
    "# Use both labeled and unlabeled training sets to improve word2vec model accuracy.\n",
    "comments = []\n",
    "   \n",
    "for i in range(0, len(X_train)):\n",
    "    comments.append(X_train[i])\n",
    "\n",
    "for i in range(0, len(Z_train)):\n",
    "    comments.append(Z_train[i])\n",
    "\n",
    "\n",
    "print(\"Unlabeled train size: {}\".format(unlabeled_train.shape[0]))\n",
    "print(\"Labeled train size: {}\".format(X_train.shape[0]))\n",
    "print(\"Labeled test size: {}\".format(X_train.shape[0]))\n",
    "print(\"Word2Vec train size: {}\".format(len(comments)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Positive Review Samples ==========\n",
      "1 \t Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
      "1 \t Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they'll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it's like to be homeless? That is Goddard Bolt's lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet's on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can't step off the sidewalk. He's given the nickname Pepto by a vagrant after it's written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They're survivors. Bolt isn't. He's not used to reaching mutual agreements like he once did when being rich where it's fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn't necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it's like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don't know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others.\n",
      "1 \t Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I'm a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often).\n",
      "1 \t This is easily the most underrated film inn the Brooks cannon. Sure, its flawed. It does not give a realistic view of homelessness (unlike, say, how Citizen Kane gave a realistic view of lounge singers, or Titanic gave a realistic view of Italians YOU IDIOTS). Many of the jokes fall flat. But still, this film is very lovable in a way many comedies are not, and to pull that off in a story about some of the most traditionally reviled members of society is truly impressive. Its not The Fisher King, but its not crap, either. My only complaint is that Brooks should have cast someone else in the lead (I love Mel as a Director and Writer, not so much as a lead).\n",
      "1 \t This is not the typical Mel Brooks film. It was much less slapstick than most of his movies and actually had a plot that was followable. Leslie Ann Warren made the movie, she is such a fantastic, under-rated actress. There were some moments that could have been fleshed out a bit more, and some scenes that could probably have been cut to make the room to do so, but all in all, this is worth the price to rent and see it. The acting was good overall, Brooks himself did a good job without his characteristic speaking to directly to the audience. Again, Warren was the best actor in the movie, but \"Fume\" and \"Sailor\" both played their parts well.\n",
      "1 \t This isn't the comedic Robin Williams, nor is it the quirky/insane Robin Williams of recent thriller fame. This is a hybrid of the classic drama without over-dramatization, mixed with Robin's new love of the thriller. But this isn't a thriller, per se. This is more a mystery/suspense vehicle through which Williams attempts to locate a sick boy and his keeper.<br /><br />Also starring Sandra Oh and Rory Culkin, this Suspense Drama plays pretty much like a news report, until William's character gets close to achieving his goal.<br /><br />I must say that I was highly entertained, though this movie fails to teach, guide, inspect, or amuse. It felt more like I was watching a guy (Williams), as he was actually performing the actions, from a third person perspective. In other words, it felt real, and I was able to subscribe to the premise of the story.<br /><br />All in all, it's worth a watch, though it's definitely not Friday/Saturday night fare.<br /><br />It rates a 7.7/10 from...<br /><br />the Fiend :.\n",
      "1 \t Yes its an art... to successfully make a slow paced thriller.<br /><br />The story unfolds in nice volumes while you don't even notice it happening.<br /><br />Fine performance by Robin Williams. The sexuality angles in the film can seem unnecessary and can probably affect how much you enjoy the film. However, the core plot is very engaging. The movie doesn't rush onto you and still grips you enough to keep you wondering. The direction is good. Use of lights to achieve desired affects of suspense and unexpectedness is good.<br /><br />Very nice 1 time watch if you are looking to lay back and hear a thrilling short story!\n",
      "1 \t In this \"critically acclaimed psychological thriller based on true events, Gabriel (Robin Williams), a celebrated writer and late-night talk show host, becomes captivated by the harrowing story of a young listener and his adoptive mother (Toni Collette). When troubling questions arise about this boy's (story), however, Gabriel finds himself drawn into a widening mystery that hides a deadly secret",
      "\" according to film's official synopsis.<br /><br />You really should STOP reading these comments, and watch the film NOW...<br /><br />The \"How did he lose his leg?\" ending, with Ms. Collette planning her new life, should be chopped off, and sent to \"deleted scenes\" land. It's overkill. The true nature of her physical and mental ailments should be obvious, by the time Mr. Williams returns to New York. Possibly, her blindness could be in question - but a revelation could have be made certain in either the \"highway\" or \"video tape\" scenes. The film would benefit from a re-editing - how about a \"director's cut\"? <br /><br />Williams and Bobby Cannavale (as Jess) don't seem, initially, believable as a couple. A scene or two establishing their relationship might have helped set the stage. Otherwise, the cast is exemplary. Williams offers an exceptionally strong characterization, and not a \"gay impersonation\". Sandra Oh (as Anna), Joe Morton (as Ashe), and Rory Culkin (Pete Logand) are all perfect.<br /><br />Best of all, Collette's \"Donna\" belongs in the creepy hall of fame. Ms. Oh is correct in saying Collette might be, \"you know, like that guy from 'Psycho'.\" There have been several years when organizations giving acting awards seemed to reach for women, due to a slighter dispersion of roles; certainly, they could have noticed Collette with some award consideration. She is that good. And, director Patrick Stettner definitely evokes Hitchcock - he even makes getting a sandwich from a vending machine suspenseful.<br /><br />Finally, writers Stettner, Armistead Maupin, and Terry Anderson deserve gratitude from flight attendants everywhere.<br /><br />******* The Night Listener (1/21/06) Patrick Stettner ~ Robin Williams, Toni Collette, Sandra Oh, Rory Culkin\n",
      "1 \t THE NIGHT LISTENER (2006) **1/2 Robin Williams, Toni Collette, Bobby Cannavale, Rory Culkin, Joe Morton, Sandra Oh, John Cullum, Lisa Emery, Becky Ann Baker. (Dir: Patrick Stettner) <br /><br />Hitchcockian suspenser gives Williams a stand-out low-key performance.<br /><br />What is it about celebrities and fans? What is the near paranoia one associates with the other and why is it almost the norm? <br /><br />In the latest derange fan scenario, based on true events no less, Williams stars as a talk-radio personality named Gabriel No one, who reads stories he's penned over the airwaves and has accumulated an interesting fan in the form of a young boy named Pete Logand (Culkin) who has submitted a manuscript about the travails of his troubled youth to No one's editor Ashe (Morton) who gives it to No one to read for himself. <br /><br />No one is naturally disturbed but ultimately intrigued about the nightmarish existence of Pete being abducted and sexually abused for years until he was finally rescued by a nurse named Donna (Collette giving an excellent performance) who has adopted the boy but her correspondence with No one reveals that Pete is dying from AIDS. Naturally No one wants to meet the fans but is suddenly in doubt to their possibly devious ulterior motives when the seed is planted by his estranged lover Jess (Cannavale) whose sudden departure from their New York City apartment has No one in an emotional tailspin that has only now grown into a tempest in a teacup when he decides to do some investigating into Donna and Pete's backgrounds discovering some truths that he didn't anticipate.<br /><br />Written by Armistead Maupin (who co-wrote the screenplay with his former lover Terry Anderson and the film's novice director Stettner) and based on a true story about a fan's hoax found out has some Hitchcockian moments that run on full tilt like any good old fashioned pot-boiler does. It helps that Williams gives a stand-out, low-key performance as the conflicted good-hearted personality who genuinely wants to believe that his number one fan is in fact real and does love him (the one thing that has escaped his own reality) and has some unsettling dreadful moments with the creepy Collette whose one physical trait I will leave unmentioned but underlines the desperation of her character that can rattle you to the core.<br /><br />However the film runs out of gas and eventually becomes a bit repetitive and predictable despite a finely directed piece of hoodwink and mystery by Stettner, it pays to listen to your own inner voice: be careful of what you hope for.\n",
      "1 \t You know, Robin Williams, God bless him, is constantly shooting himself in the foot lately with all these dumb comedies he has done this decade (with perhaps the exception of \"Death To Smoochy\", which bombed when it came out but is now a cult classic). The dramas he has made lately have been fantastic, especially \"Insomnia\" and \"One Hour Photo\". \"The Night Listener\", despite mediocre reviews and a quick DVD release, is among his best work, period.<br /><br />This is a very chilling story, even though it doesn't include a serial killer or anyone that physically dangerous for that matter. The concept of the film is based on an actual case of fraud that still has yet to be officially confirmed. In high school, I read an autobiography by a child named Anthony Godby Johnson, who suffered horrific abuse and eventually contracted AIDS as a result. I was moved by the story until I read reports online that Johnson may not actually exist. When I saw this movie, the confused feelings that Robin Williams so brilliantly portrayed resurfaced in my mind.<br /><br />Toni Collette probably gives her best dramatic performance too as the ultimately sociopathic \"caretaker\". Her role was a far cry from those she had in movies like \"Little Miss Sunshine\". There were even times she looked into the camera where I thought she was staring right at me. It takes a good actress to play that sort of role, and it's this understated (yet well reviewed) role that makes Toni Collette probably one of the best actresses of this generation not to have even been nominated for an Academy Award (as of 2008). It's incredible that there is at least one woman in this world who is like this, and it's scary too.<br /><br />This is a good, dark film that I highly recommend. Be prepared to be unsettled, though, because this movie leaves you with a strange feeling at the end.\n",
      "========== Negative Review Samples ==========\n",
      "0 \t Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\n",
      "0 \t Airport '77 starts as a brand new luxury 747 plane is loaded up with valuable paintings & such belonging to rich businessman Philip Stevens (James Stewart) who is flying them & a bunch of VIP's to his estate in preparation of it being opened to the public as a museum, also on board is Stevens daughter Julie (Kathleen Quinlan) & her son. The luxury jetliner takes off as planned but mid-air the plane is hi-jacked by the co-pilot Chambers (Robert Foxworth) & his two accomplice's Banker (Monte Markham) & Wilson (Michael Pataki) who knock the passengers & crew out with sleeping gas, they plan to steal the valuable cargo & land on a disused plane strip on an isolated island but while making his descent Chambers almost hits an oil rig in the Ocean & loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the Bermuda Triangle. With air in short supply, water leaking in & having flown over 200 miles off course the problems mount for the survivor's as they await help with time fast running out...<br /><br />Also known under the slightly different tile Airport 1977 this second sequel to the smash-hit disaster thriller Airport (1970) was directed by Jerry Jameson & while once again like it's predecessors I can't say Airport '77 is any sort of forgotten classic it is entertaining although not necessarily for the right reasons. Out of the three Airport films I have seen so far I actually liked this one the best, just. It has my favourite plot of the three with a nice mid-air hi-jacking & then the crashing (didn't he see the oil rig?) & sinking of the 747 (maybe the makers were trying to cross the original Airport with another popular disaster flick of the period The Poseidon Adventure (1972)) & submerged is where it stays until the end with a stark dilemma facing those trapped inside, either suffocate when the air runs out or drown as the 747 floods or if any of the doors are opened & it's a decent idea that could have made for a great little disaster flick but bad unsympathetic character's, dull dialogue, lethargic set-pieces & a real lack of danger or suspense or tension means this is a missed opportunity. While the rather sluggish plot keeps one entertained for 108 odd minutes not that much happens after the plane sinks & there's not as much urgency as I thought there should have been. Even when the Navy become involved things don't pick up that much with a few shots of huge ships & helicopters flying about but there's just something lacking here. George Kennedy as the jinxed airline worker Joe Patroni is back but only gets a couple of scenes & barely even says anything preferring to just look worried in the background.<br /><br />The home video & theatrical version of Airport '77 run 108 minutes while the US TV versions add an extra hour of footage including a new opening credits sequence, many more scenes with George Kennedy as Patroni, flashbacks to flesh out character's, longer rescue scenes & the discovery or another couple of dead bodies including the navigator. While I would like to see this extra footage I am not sure I could sit through a near three hour cut of Airport '77. As expected the film has dated badly with horrible fashions & interior design choices, I will say no more other than the toy plane model effects aren't great either. Along with the other two Airport sequels this takes pride of place in the Razzie Award's Hall of Shame although I can think of lots of worse films than this so I reckon that's a little harsh. The action scenes are a little dull unfortunately, the pace is slow & not much excitement or tension is generated which is a shame as I reckon this could have been a pretty good film if made properly.<br /><br />The production values are alright if nothing spectacular. The acting isn't great, two time Oscar winner Jack Lemmon has said since it was a mistake to star in this, one time Oscar winner James Stewart looks old & frail, also one time Oscar winner Lee Grant looks drunk while Sir Christopher Lee is given little to do & there are plenty of other familiar faces to look out for too.<br /><br />Airport '77 is the most disaster orientated of the three Airport films so far & I liked the ideas behind it even if they were a bit silly, the production & bland direction doesn't help though & a film about a sunken plane just shouldn't be this boring or lethargic. Followed by The Concorde ... Airport '79 (1979).\n",
      "0 \t This film lacked something I couldn't put my finger on at first: charisma on the part of the leading actress. This inevitably translated to lack of chemistry when she shared the screen with her leading man. Even the romantic scenes came across as being merely the actors at play. It could very well have been the director who miscalculated what he needed from the actors. I just don't know.<br /><br />But could it have been the screenplay? Just exactly who was the chef in love with? He seemed more enamored of his culinary skills and restaurant, and ultimately of himself and his youthful exploits, than of anybody or anything else. He never convinced me he was in love with the princess.<br /><br />I was disappointed in this movie. But, don't forget it was nominated for an Oscar, so judge for yourself.\n",
      "0 \t Sorry everyone,,, I know this is supposed to be an \"art\" film,, but wow, they should have handed out guns at the screening so people could blow their brains out and not watch. Although the scene design and photographic direction was excellent, this story is too painful to watch. The absence of a sound track was brutal. The loooonnnnng shots were too long. How long can you watch two people just sitting there and talking? Especially when the dialogue is two people complaining. I really had a hard time just getting through this film. The performances were excellent, but how much of that dark, sombre, uninspired, stuff can you take? The only thing i liked was Maureen Stapleton and her red dress and dancing scene. Otherwise this was a ripoff of Bergman. And i'm no fan f his either. I think anyone who says they enjoyed 1 1/2 hours of this is,, well, lying.\n",
      "0 \t When I was little my parents took me along to the theater to see Interiors. It was one of many movies I watched with my parents, but this was the only one we walked out of. Since then I had never seen Interiors until just recently, and I could have lived out the rest of my life without it. What a pretentious, ponderous, and painfully boring piece of 70's wine and cheese tripe. Woody Allen is one of my favorite directors but Interiors is by far the worst piece of crap of his career. In the unmistakable style of Ingmar Berman, Allen gives us a dark, angular, muted, insight in to the lives of a family wrought by the psychological damage caused by divorce, estrangement, career, love, non-love, halitosis, whatever. The film, intentionally, has no comic relief, no music, and is drenched in shadowy pathos. This film style can be best defined as expressionist in nature, using an improvisational method of dialogue to illicit a \"more pronounced depth of meaning and truth\". But Woody Allen is no Ingmar Bergman. The film is painfully slow and dull. But beyond that, I simply had no connection with or sympathy for any of the characters. Instead I felt only contempt for this parade of shuffling, whining, nicotine stained, martyrs in a perpetual quest for identity. Amid a backdrop of cosmopolitan affluence and baked Brie intelligentsia the story looms like a fart in the room. Everyone speaks in affected platitudes and elevated language between cigarettes. Everyone is \"lost\" and \"struggling\", desperate to find direction or understanding or whatever and it just goes on and on to the point where you just want to slap all of them. It's never about resolution, it's only about interminable introspective babble. It is nothing more than a psychological drama taken to an extreme beyond the audience's ability to connect. Woody Allen chose to make characters so immersed in themselves we feel left out. And for that reason I found this movie painfully self indulgent and spiritually draining. I see what he was going for but his insistence on promoting his message through Prozac prose and distorted film techniques jettisons it past the point of relevance. I highly recommend this one if you're feeling a little too happy and need something to remind you of death. Otherwise, let's just pretend this film never happened.\n",
      "0 \t It appears that many critics find the idea of a Woody Allen drama unpalatable. And for good reason: they are unbearably wooden and pretentious imitations of Bergman. And let's not kid ourselves: critics were mostly supportive of Allen's Bergman pretensions, Allen's whining accusations to the contrary notwithstanding. What I don't get is this: why was Allen generally applauded for his originality in imitating Bergman, but the contemporaneous Brian DePalma was excoriated for \"ripping off\" Hitchcock in his suspense/horror films? In Robin Wood's view, it's a strange form of cultural snobbery. I would have to agree with that.\n",
      "0 \t The plot for Descent, if it actually can be called a plot, has two noteworthy events. One near the beginning - one at the end. Together these events make up maybe 5% of the total movie time. Everything (and I mean _everything_) in between is basically the director's desperate effort to fill in the minutes. I like disturbing movies, I like dark movies and I don't get troubled by gritty scenes - but if you expect me to sit through 60 minutes of hazy/dark (literally) scenes with NO storyline you have another thing coming. Rosario Dawson, one of my favorite actresses is completely wasted here. And no, she doesn't get naked, not even in the NC-17 version, which I saw.<br /><br />If you have a couple of hours to throw away and want to watch \"Descent\", take a nap instead - you'll probably have more interesting dreams.\n",
      "0 \t The second attempt by a New York intellectual in less than 10 years to make a \"Swedish\" film - the first being Susan Sontag's \"Brother Carl\" (which was made in Sweden, with Swedish actors, no less!) The results? Oscar Wilde said it best, in reference to Dickens' \"The Old Curiosity Shop\": \"One would have to have a heart of stone not to laugh out loud at the death of Little Nell.\" Pretty much the same thing here. \"Interiors\" is chock full of solemnly intoned howlers. (\"I'm afraid of my anger.\" Looking into the middle distance: \"I don't like who I'm becoming.\") The directorial quotations (to use a polite term) from Bergman are close to parody. The incredibly self-involved family keep reminding us of how brilliant and talented they are, to the point of strangulation. (\"I read a poem of yours the other day. It was in - I don't know - The New Yorker.\" \"Oh. That was an old poem. I reworked it.\") Far from not caring about these people, however, I found them quite hilarious. Much of the dialog is exactly like the funny stuff from Allen's earlier films - only he's directed his actors to play the lines straight. Having not cast himself in the movie, he has poor Mary Beth Hurt copy all of his thespian tics, intonations, and neurotic habits, turning her into an embarrassing surrogate (much like Kenneth Branagh in \"Celebrity\").<br /><br />The basic plot - dysfunctional family with quietly domineering mother - seems to be lifted more or less from Bergman's \"Winter Light,\" the basic family melodrama tricked up with a lot of existential angst. It all comes through in the shopworn visual/aural tricks: the deafening scratching of a pencil on paper, the towering surf that dwarfs the people walking on the beach. etc, etc.<br /><br />Allen's later \"serious\" films are less embarrassing, but also far less entertaining. I'll take \"Interiors.\" Woody's rarely made a funnier movie.\n",
      "0 \t I don't know who to blame, the timid writers or the clueless director. It seemed to be one of those movies where so much was paid to the stars (Angie, Charlie, Denise, Rosanna and Jon) that there wasn't enough left to really make a movie. This could have been very entertaining, but there was a veil of timidity, even cowardice, that hung over each scene. Since it got an R rating anyway why was the ubiquitous bubble bath scene shot with a 70-year-old woman and not Angie Harmon? Why does Sheen sleepwalk through potentially hot relationships WITH TWO OF THE MOST BEAUTIFUL AND SEXY ACTRESSES in the world? If they were only looking for laughs why not cast Whoopi Goldberg and Judy Tenuta instead? This was so predictable I was surprised to find that the director wasn't a five year old. What a waste, not just for the viewers but for the actors as well.\n",
      "0 \t This film is mediocre at best. Angie Harmon is as funny as a bag of hammers. Her bitchy demeanor from \"Law and Order\" carries over in a failed attempt at comedy. Charlie Sheen is the only one to come out unscathed in this horrible anti-comedy. The only positive thing to come out of this mess is Charlie and Denise's marriage. Hopefully that effort produces better results.\n"
     ]
    }
   ],
   "source": [
    "print(\"========== Positive Review Samples ==========\")\n",
    "\n",
    "for i in range(0, 10):\n",
    "    print(\"{} \\t {}\".format(labeled_train['sentiment'][i], labeled_train['comment'][i]))\n",
    "    \n",
    "print(\"========== Negative Review Samples ==========\")\n",
    "\n",
    "for j in range(12500, 12510):\n",
    "    print(\"{} \\t {}\".format(labeled_train['sentiment'][j], labeled_train['comment'][j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Count Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Train Statistics =====\n",
      "Min: 5\n",
      "Max: 2470\n",
      "Mean: 233.51256\n",
      "Median: 174.0\n",
      "Std: 173.36922103489533\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Count number of tokens for each comment for training data\n",
    "trainLens = []\n",
    "\n",
    "for i in range(0, X_train.shape[0]):\n",
    "    trainLens.append(len(labeled_train['comment'][i].split()))\n",
    "\n",
    "print(\"===== Train Statistics =====\")\n",
    "print(\"Min: {}\".format(np.amin(trainLens)))\n",
    "print(\"Max: {}\".format(np.amax(trainLens)))\n",
    "print(\"Mean: {}\".format(np.mean(trainLens)))\n",
    "print(\"Median: {}\".format(np.median(trainLens)))\n",
    "print(\"Std: {}\".format(np.std(trainLens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common 50 Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1013857),\n",
       " ('and', 493424),\n",
       " ('of', 439103),\n",
       " ('to', 406655),\n",
       " ('is', 321239),\n",
       " ('br', 306353),\n",
       " ('it', 287579),\n",
       " ('in', 282460),\n",
       " ('this', 226547),\n",
       " ('that', 218562),\n",
       " ('was', 144097),\n",
       " ('as', 139772),\n",
       " ('with', 133669),\n",
       " ('for', 133469),\n",
       " ('movie', 132122),\n",
       " ('but', 126814),\n",
       " ('film', 121074),\n",
       " ('you', 103731),\n",
       " ('on', 101756),\n",
       " ('not', 90723),\n",
       " ('he', 89881),\n",
       " ('are', 89063),\n",
       " ('his', 87763),\n",
       " ('have', 83511),\n",
       " ('be', 81470),\n",
       " ('one', 81220),\n",
       " ('all', 71046),\n",
       " ('at', 69981),\n",
       " ('they', 69879),\n",
       " ('by', 68133),\n",
       " ('who', 65078),\n",
       " ('an', 64962),\n",
       " ('from', 62325),\n",
       " ('so', 61943),\n",
       " ('like', 60925),\n",
       " ('there', 57259),\n",
       " ('her', 54495),\n",
       " ('or', 54161),\n",
       " ('just', 53550),\n",
       " ('about', 52042),\n",
       " ('out', 51384),\n",
       " ('has', 50718),\n",
       " ('if', 50464),\n",
       " ('what', 48320),\n",
       " ('some', 47775),\n",
       " ('good', 44843),\n",
       " ('can', 43900),\n",
       " ('more', 42616),\n",
       " ('when', 42563),\n",
       " ('very', 41810)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "wordCounter = Counter()\n",
    "\n",
    "for i in range(0, len(comments)):\n",
    "    wordCounter.update(comments[i])\n",
    "\n",
    "wordCounter.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smiley Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 29, 0: 21})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Keep track the number of smiley faces in each label\n",
    "smileyCounter = Counter()\n",
    "\n",
    "for i in range(0, len(labeled_train['comment'])):\n",
    "    if \":-)\" in labeled_train['comment'][i]: \n",
    "        smileyCounter[y_train[i]] += 1\n",
    "    \n",
    "print(smileyCounter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the distribution of 200 most common words\n",
    "labels, values = zip(*wordCounter.most_common(200))\n",
    "indexes = np.arange(len(labels))\n",
    "width = 1\n",
    "\n",
    "plt.title(\"Frequency of 200 Most Common Words\")\n",
    "plt.xlabel('frequency rank of word')\n",
    "plt.ylabel('frequency')\n",
    "plt.bar(indexes, values, width)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.utils import simple_preprocess\n",
    "from collections import defaultdict\n",
    "\n",
    "class MeanWordVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(word2vec.wv.get_vector('incredible'))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec.wv.vocab]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_default = []\n",
    "\n",
    "for i in range(0, len(comments)):\n",
    "    documents_default.append([ w for w in comments[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-01 20:31:59,760 : INFO : collecting all words and their counts\n",
      "2018-09-01 20:31:59,761 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-09-01 20:32:00,114 : INFO : PROGRESS: at sentence #10000, processed 2280736 words, keeping 49391 word types\n",
      "2018-09-01 20:32:00,467 : INFO : PROGRESS: at sentence #20000, processed 4511766 words, keeping 66818 word types\n",
      "2018-09-01 20:32:00,836 : INFO : PROGRESS: at sentence #30000, processed 6775545 words, keeping 79713 word types\n",
      "2018-09-01 20:32:01,212 : INFO : PROGRESS: at sentence #40000, processed 9081000 words, keeping 90678 word types\n",
      "2018-09-01 20:32:01,582 : INFO : PROGRESS: at sentence #50000, processed 11352902 words, keeping 100450 word types\n",
      "2018-09-01 20:32:01,948 : INFO : PROGRESS: at sentence #60000, processed 13579732 words, keeping 108925 word types\n",
      "2018-09-01 20:32:02,312 : INFO : PROGRESS: at sentence #70000, processed 15847194 words, keeping 117057 word types\n",
      "2018-09-01 20:32:02,494 : INFO : collected 120929 word types from a corpus of 16974301 raw words and 75000 sentences\n",
      "2018-09-01 20:32:02,495 : INFO : Loading a fresh vocabulary\n",
      "2018-09-01 20:32:02,609 : INFO : min_count=5 retains 47063 unique words (38% of original 120929, drops 73866)\n",
      "2018-09-01 20:32:02,610 : INFO : min_count=5 leaves 16855091 word corpus (99% of original 16974301, drops 119210)\n",
      "2018-09-01 20:32:02,733 : INFO : deleting the raw counts dictionary of 120929 items\n",
      "2018-09-01 20:32:02,736 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2018-09-01 20:32:02,737 : INFO : downsampling leaves estimated 12837487 word corpus (76.2% of prior 16855091)\n",
      "2018-09-01 20:32:02,891 : INFO : estimated required memory for 47063 words and 100 dimensions: 61181900 bytes\n",
      "2018-09-01 20:32:02,892 : INFO : resetting layer weights\n",
      "2018-09-01 20:32:03,305 : INFO : training model with 3 workers on 47063 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-09-01 20:32:04,313 : INFO : EPOCH 1 - PROGRESS: at 9.59% examples, 1227676 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:32:05,315 : INFO : EPOCH 1 - PROGRESS: at 19.42% examples, 1243724 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:06,315 : INFO : EPOCH 1 - PROGRESS: at 30.13% examples, 1286091 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:07,323 : INFO : EPOCH 1 - PROGRESS: at 39.51% examples, 1259526 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:08,325 : INFO : EPOCH 1 - PROGRESS: at 49.81% examples, 1278427 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:09,329 : INFO : EPOCH 1 - PROGRESS: at 60.64% examples, 1295678 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:10,333 : INFO : EPOCH 1 - PROGRESS: at 71.49% examples, 1307238 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:11,336 : INFO : EPOCH 1 - PROGRESS: at 82.30% examples, 1316880 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:32:12,339 : INFO : EPOCH 1 - PROGRESS: at 93.03% examples, 1323835 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:32:12,978 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-01 20:32:12,980 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-01 20:32:12,982 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-01 20:32:12,982 : INFO : EPOCH - 1 : training on 16974301 raw words (12837929 effective words) took 9.7s, 1327208 effective words/s\n",
      "2018-09-01 20:32:13,991 : INFO : EPOCH 2 - PROGRESS: at 8.33% examples, 1069556 words/s, in_qsize 4, out_qsize 1\n",
      "2018-09-01 20:32:14,993 : INFO : EPOCH 2 - PROGRESS: at 19.12% examples, 1223015 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:15,995 : INFO : EPOCH 2 - PROGRESS: at 30.03% examples, 1278847 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:16,999 : INFO : EPOCH 2 - PROGRESS: at 40.86% examples, 1303518 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:32:18,000 : INFO : EPOCH 2 - PROGRESS: at 51.39% examples, 1319900 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:19,009 : INFO : EPOCH 2 - PROGRESS: at 62.22% examples, 1329087 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:20,011 : INFO : EPOCH 2 - PROGRESS: at 72.60% examples, 1326706 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:21,016 : INFO : EPOCH 2 - PROGRESS: at 82.86% examples, 1325284 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:22,028 : INFO : EPOCH 2 - PROGRESS: at 93.85% examples, 1332320 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:22,703 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-01 20:32:22,708 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-01 20:32:22,713 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-01 20:32:22,713 : INFO : EPOCH - 2 : training on 16974301 raw words (12838539 effective words) took 9.7s, 1319600 effective words/s\n",
      "2018-09-01 20:32:23,724 : INFO : EPOCH 3 - PROGRESS: at 10.66% examples, 1365013 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:24,725 : INFO : EPOCH 3 - PROGRESS: at 21.19% examples, 1354005 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:25,726 : INFO : EPOCH 3 - PROGRESS: at 30.57% examples, 1304542 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:26,727 : INFO : EPOCH 3 - PROGRESS: at 40.86% examples, 1305417 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:27,727 : INFO : EPOCH 3 - PROGRESS: at 51.15% examples, 1313992 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:28,734 : INFO : EPOCH 3 - PROGRESS: at 61.93% examples, 1324795 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:29,735 : INFO : EPOCH 3 - PROGRESS: at 72.80% examples, 1331739 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:30,737 : INFO : EPOCH 3 - PROGRESS: at 83.27% examples, 1333711 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:31,738 : INFO : EPOCH 3 - PROGRESS: at 92.73% examples, 1321032 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:32,498 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-01 20:32:32,500 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-01 20:32:32,508 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-01 20:32:32,509 : INFO : EPOCH - 3 : training on 16974301 raw words (12836881 effective words) took 9.8s, 1311070 effective words/s\n",
      "2018-09-01 20:32:33,516 : INFO : EPOCH 4 - PROGRESS: at 8.54% examples, 1102680 words/s, in_qsize 6, out_qsize 1\n",
      "2018-09-01 20:32:34,520 : INFO : EPOCH 4 - PROGRESS: at 19.06% examples, 1220289 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:32:35,523 : INFO : EPOCH 4 - PROGRESS: at 28.85% examples, 1226929 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:36,532 : INFO : EPOCH 4 - PROGRESS: at 38.64% examples, 1228090 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:37,534 : INFO : EPOCH 4 - PROGRESS: at 48.97% examples, 1254631 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:38,536 : INFO : EPOCH 4 - PROGRESS: at 59.24% examples, 1265287 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:39,538 : INFO : EPOCH 4 - PROGRESS: at 70.11% examples, 1282328 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:40,544 : INFO : EPOCH 4 - PROGRESS: at 80.32% examples, 1283539 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:41,545 : INFO : EPOCH 4 - PROGRESS: at 90.40% examples, 1286413 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:42,426 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-01 20:32:42,427 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-01 20:32:42,433 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-01 20:32:42,433 : INFO : EPOCH - 4 : training on 16974301 raw words (12838575 effective words) took 9.9s, 1294127 effective words/s\n",
      "2018-09-01 20:32:43,441 : INFO : EPOCH 5 - PROGRESS: at 8.02% examples, 1034015 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:44,450 : INFO : EPOCH 5 - PROGRESS: at 17.78% examples, 1138413 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:45,458 : INFO : EPOCH 5 - PROGRESS: at 26.78% examples, 1133315 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:32:46,466 : INFO : EPOCH 5 - PROGRESS: at 37.47% examples, 1187637 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:47,477 : INFO : EPOCH 5 - PROGRESS: at 47.50% examples, 1212873 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-01 20:32:48,479 : INFO : EPOCH 5 - PROGRESS: at 57.31% examples, 1220694 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:49,484 : INFO : EPOCH 5 - PROGRESS: at 67.39% examples, 1230609 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:32:50,491 : INFO : EPOCH 5 - PROGRESS: at 77.74% examples, 1237898 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:32:51,494 : INFO : EPOCH 5 - PROGRESS: at 87.87% examples, 1246358 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:52,502 : INFO : EPOCH 5 - PROGRESS: at 97.67% examples, 1245455 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:52,780 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-01 20:32:52,784 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-01 20:32:52,785 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-01 20:32:52,786 : INFO : EPOCH - 5 : training on 16974301 raw words (12837961 effective words) took 10.4s, 1240285 effective words/s\n",
      "2018-09-01 20:32:52,786 : INFO : training on a 84871505 raw words (64189885 effective words) took 49.5s, 1297259 effective words/s\n",
      "2018-09-01 20:32:52,787 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2018-09-01 20:32:52,788 : INFO : training model with 3 workers on 47063 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-09-01 20:32:53,798 : INFO : EPOCH 1 - PROGRESS: at 13.03% examples, 1282130 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:32:54,799 : INFO : EPOCH 1 - PROGRESS: at 26.55% examples, 1291251 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:55,802 : INFO : EPOCH 1 - PROGRESS: at 40.64% examples, 1319408 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:56,803 : INFO : EPOCH 1 - PROGRESS: at 54.40% examples, 1333657 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:32:57,806 : INFO : EPOCH 1 - PROGRESS: at 68.16% examples, 1335818 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:32:58,818 : INFO : EPOCH 1 - PROGRESS: at 81.38% examples, 1322663 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:32:59,828 : INFO : EPOCH 1 - PROGRESS: at 92.01% examples, 1281559 words/s, in_qsize 5, out_qsize 1\n",
      "2018-09-01 20:33:00,405 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-01 20:33:00,412 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-01 20:33:00,421 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-01 20:33:00,422 : INFO : EPOCH - 1 : training on 9908215 raw words (9789005 effective words) took 7.6s, 1282973 effective words/s\n",
      "2018-09-01 20:33:01,426 : INFO : EPOCH 2 - PROGRESS: at 11.72% examples, 1151001 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:33:02,451 : INFO : EPOCH 2 - PROGRESS: at 23.12% examples, 1114711 words/s, in_qsize 4, out_qsize 1\n",
      "2018-09-01 20:33:03,455 : INFO : EPOCH 2 - PROGRESS: at 36.00% examples, 1158570 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:04,463 : INFO : EPOCH 2 - PROGRESS: at 49.56% examples, 1206046 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:05,463 : INFO : EPOCH 2 - PROGRESS: at 62.33% examples, 1215197 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:06,463 : INFO : EPOCH 2 - PROGRESS: at 75.01% examples, 1216288 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:33:07,474 : INFO : EPOCH 2 - PROGRESS: at 88.97% examples, 1237602 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:08,273 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-01 20:33:08,280 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-01 20:33:08,290 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-01 20:33:08,291 : INFO : EPOCH - 2 : training on 9908215 raw words (9789005 effective words) took 7.9s, 1244477 effective words/s\n",
      "2018-09-01 20:33:09,298 : INFO : EPOCH 3 - PROGRESS: at 11.88% examples, 1167066 words/s, in_qsize 4, out_qsize 1\n",
      "2018-09-01 20:33:10,303 : INFO : EPOCH 3 - PROGRESS: at 24.96% examples, 1211941 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:11,303 : INFO : EPOCH 3 - PROGRESS: at 38.02% examples, 1231520 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:12,304 : INFO : EPOCH 3 - PROGRESS: at 49.45% examples, 1211723 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:13,309 : INFO : EPOCH 3 - PROGRESS: at 62.54% examples, 1224453 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:14,313 : INFO : EPOCH 3 - PROGRESS: at 77.00% examples, 1252626 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:33:15,317 : INFO : EPOCH 3 - PROGRESS: at 91.29% examples, 1274153 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:15,892 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-01 20:33:15,903 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-01 20:33:15,906 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-01 20:33:15,906 : INFO : EPOCH - 3 : training on 9908215 raw words (9789005 effective words) took 7.6s, 1285909 effective words/s\n",
      "2018-09-01 20:33:16,918 : INFO : EPOCH 4 - PROGRESS: at 13.62% examples, 1339023 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:17,918 : INFO : EPOCH 4 - PROGRESS: at 27.79% examples, 1354147 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:18,932 : INFO : EPOCH 4 - PROGRESS: at 42.20% examples, 1366138 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:19,939 : INFO : EPOCH 4 - PROGRESS: at 52.82% examples, 1289136 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:20,941 : INFO : EPOCH 4 - PROGRESS: at 65.76% examples, 1285051 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:21,943 : INFO : EPOCH 4 - PROGRESS: at 80.39% examples, 1304979 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:22,949 : INFO : EPOCH 4 - PROGRESS: at 94.85% examples, 1320128 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:23,286 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-01 20:33:23,298 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-01 20:33:23,302 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-01 20:33:23,303 : INFO : EPOCH - 4 : training on 9908215 raw words (9789005 effective words) took 7.4s, 1324378 effective words/s\n",
      "2018-09-01 20:33:24,306 : INFO : EPOCH 5 - PROGRESS: at 13.03% examples, 1288524 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:33:25,316 : INFO : EPOCH 5 - PROGRESS: at 27.53% examples, 1337310 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:26,320 : INFO : EPOCH 5 - PROGRESS: at 41.73% examples, 1352777 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:33:27,324 : INFO : EPOCH 5 - PROGRESS: at 55.46% examples, 1358036 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:28,328 : INFO : EPOCH 5 - PROGRESS: at 69.65% examples, 1360815 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:29,333 : INFO : EPOCH 5 - PROGRESS: at 83.95% examples, 1364512 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:30,336 : INFO : EPOCH 5 - PROGRESS: at 97.80% examples, 1361825 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:30,467 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-01 20:33:30,477 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-01 20:33:30,482 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-01 20:33:30,483 : INFO : EPOCH - 5 : training on 9908215 raw words (9789005 effective words) took 7.2s, 1363882 effective words/s\n",
      "2018-09-01 20:33:31,491 : INFO : EPOCH 6 - PROGRESS: at 14.39% examples, 1422262 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:32,492 : INFO : EPOCH 6 - PROGRESS: at 28.84% examples, 1404978 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:33,497 : INFO : EPOCH 6 - PROGRESS: at 40.73% examples, 1322946 words/s, in_qsize 6, out_qsize 1\n",
      "2018-09-01 20:33:34,507 : INFO : EPOCH 6 - PROGRESS: at 52.31% examples, 1279910 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:35,509 : INFO : EPOCH 6 - PROGRESS: at 66.65% examples, 1304846 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:36,511 : INFO : EPOCH 6 - PROGRESS: at 80.87% examples, 1315142 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:37,515 : INFO : EPOCH 6 - PROGRESS: at 92.51% examples, 1290264 words/s, in_qsize 6, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-01 20:33:38,054 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-01 20:33:38,059 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-01 20:33:38,070 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-01 20:33:38,071 : INFO : EPOCH - 6 : training on 9908215 raw words (9789005 effective words) took 7.6s, 1291054 effective words/s\n",
      "2018-09-01 20:33:39,076 : INFO : EPOCH 7 - PROGRESS: at 12.95% examples, 1274865 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:40,089 : INFO : EPOCH 7 - PROGRESS: at 26.03% examples, 1261036 words/s, in_qsize 5, out_qsize 2\n",
      "2018-09-01 20:33:41,089 : INFO : EPOCH 7 - PROGRESS: at 38.13% examples, 1231924 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:42,093 : INFO : EPOCH 7 - PROGRESS: at 51.25% examples, 1252568 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:43,095 : INFO : EPOCH 7 - PROGRESS: at 62.21% examples, 1217089 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:44,098 : INFO : EPOCH 7 - PROGRESS: at 76.52% examples, 1243053 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:45,111 : INFO : EPOCH 7 - PROGRESS: at 90.59% examples, 1261640 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:45,738 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-01 20:33:45,751 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-01 20:33:45,756 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-01 20:33:45,757 : INFO : EPOCH - 7 : training on 9908215 raw words (9789005 effective words) took 7.7s, 1273954 effective words/s\n",
      "2018-09-01 20:33:46,761 : INFO : EPOCH 8 - PROGRESS: at 10.78% examples, 1062566 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:33:47,770 : INFO : EPOCH 8 - PROGRESS: at 24.15% examples, 1171598 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:48,774 : INFO : EPOCH 8 - PROGRESS: at 36.78% examples, 1190348 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:49,790 : INFO : EPOCH 8 - PROGRESS: at 50.93% examples, 1241988 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:50,791 : INFO : EPOCH 8 - PROGRESS: at 64.45% examples, 1259407 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:51,803 : INFO : EPOCH 8 - PROGRESS: at 76.72% examples, 1242565 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:52,820 : INFO : EPOCH 8 - PROGRESS: at 89.55% examples, 1243782 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:33:53,549 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-01 20:33:53,553 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-01 20:33:53,563 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-01 20:33:53,564 : INFO : EPOCH - 8 : training on 9908215 raw words (9789005 effective words) took 7.8s, 1254260 effective words/s\n",
      "2018-09-01 20:33:54,579 : INFO : EPOCH 9 - PROGRESS: at 13.89% examples, 1359478 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:55,586 : INFO : EPOCH 9 - PROGRESS: at 27.90% examples, 1350498 words/s, in_qsize 5, out_qsize 1\n",
      "2018-09-01 20:33:56,589 : INFO : EPOCH 9 - PROGRESS: at 39.83% examples, 1287582 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:33:57,590 : INFO : EPOCH 9 - PROGRESS: at 52.20% examples, 1275785 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:33:58,599 : INFO : EPOCH 9 - PROGRESS: at 66.53% examples, 1299662 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:33:59,610 : INFO : EPOCH 9 - PROGRESS: at 80.87% examples, 1310496 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:34:00,611 : INFO : EPOCH 9 - PROGRESS: at 95.17% examples, 1322965 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:34:00,933 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-01 20:34:00,937 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-01 20:34:00,941 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-01 20:34:00,942 : INFO : EPOCH - 9 : training on 9908215 raw words (9789005 effective words) took 7.4s, 1327275 effective words/s\n",
      "2018-09-01 20:34:01,956 : INFO : EPOCH 10 - PROGRESS: at 13.73% examples, 1340865 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:34:02,968 : INFO : EPOCH 10 - PROGRESS: at 27.31% examples, 1318813 words/s, in_qsize 5, out_qsize 1\n",
      "2018-09-01 20:34:03,969 : INFO : EPOCH 10 - PROGRESS: at 41.12% examples, 1328802 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:34:04,977 : INFO : EPOCH 10 - PROGRESS: at 54.40% examples, 1326260 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:34:05,978 : INFO : EPOCH 10 - PROGRESS: at 67.85% examples, 1324463 words/s, in_qsize 6, out_qsize 0\n",
      "2018-09-01 20:34:06,986 : INFO : EPOCH 10 - PROGRESS: at 81.79% examples, 1325462 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:34:07,990 : INFO : EPOCH 10 - PROGRESS: at 94.70% examples, 1315760 words/s, in_qsize 5, out_qsize 0\n",
      "2018-09-01 20:34:08,372 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-01 20:34:08,385 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-01 20:34:08,387 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-01 20:34:08,388 : INFO : EPOCH - 10 : training on 9908215 raw words (9789005 effective words) took 7.4s, 1315018 effective words/s\n",
      "2018-09-01 20:34:08,388 : INFO : training on a 99082150 raw words (97890050 effective words) took 75.6s, 1294835 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(97890050, 99082150)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_default = Word2Vec(documents_default)\n",
    "word2vec_default.train(documents, total_examples=len(documents_default), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vectorizer_default = MeanWordVectorizer(word2vec_default)\n",
    "X1_default = vectorizer_default.transform(X_train)\n",
    "X2_default = vectorizer_default.transform(X_test)\n",
    "\n",
    "X_train_default, X_test_default, y_train_default, y_test_default = train_test_split(X1_default, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 85.48%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_train_default, y_train_default)\n",
    "y_pred = logistic.predict(X_test_default)\n",
    "accuracy = accuracy_score(y_test_default, y_pred)\n",
    "\n",
    "print(\"Test Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.56%\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "xgboost = XGBClassifier()\n",
    "xgboost.fit(X_train_default, y_train_default)\n",
    "y_pred = xgboost.predict(X_test_default)\n",
    "accuracy = accuracy_score(y_test_default, y_pred)\n",
    "\n",
    "print(\"Test Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['homelessness', 'houselessness', 'george', 'carlin', 'stated', 'been', 'issue', 'years', 'never', 'plan', 'help', 'those', 'street', 'were', 'once', 'considered', 'human', 'did', 'everything', 'going', 'school', 'work', 'vote', 'matter', 'most', 'people', 'think', 'homeless', 'lost', 'cause', 'while', 'worrying', 'things', 'such', 'racism', 'war', 'iraq', 'pressuring', 'kids', 'succeed', 'technology', 'elections', 'inflation', 'worrying', 'll', 'next', 'end', 'up', 'streets', 'were', 'given', 'bet', 'live', 'streets', 'month', 'without', 'luxuries', 'once', 'had', 'home', 'entertainment', 'sets', 'bathroom', 'pictures', 'wall', 'computer', 'everything', 'once', 'treasure', 'see', 'homeless', 'goddard', 'bolt', 'lesson', 'mel', 'brooks', 'directs', 'stars', 'bolt', 'plays', 'rich', 'man', 'everything', 'world', 'until', 'deciding', 'make', 'bet', 'sissy', 'rival', 'jeffery', 'tambor', 'see', 'live', 'streets', 'thirty', 'days', 'without', 'luxuries', 'bolt', 'succeeds', 'do', 'wants', 'future', 'project', 'making', 'buildings', 'bet', 'where', 'bolt', 'thrown', 'street', 'bracelet', 'leg', 'monitor', 'every', 'move', 'where', 'step', 'off', 'sidewalk', 'given', 'nickname', 'pepto', 'vagrant', 'after', 'written', 'forehead', 'where', 'bolt', 'meets', 'other', 'characters', 'including', 'woman', 'name', 'molly', 'lesley', 'ann', 'warren', 'ex', 'dancer', 'got', 'divorce', 'before', 'losing', 'home', 'pals', 'sailor', 'howard', 'morris', 'fumes', 'teddy', 'wilson', 'already', 'used', 'streets', 're', 'survivors', 'bolt', 'isn', 'used', 'reaching', 'mutual', 'agreements', 'once', 'did', 'being', 'rich', 'where', 'fight', 'flight', 'kill', 'killed', 'while', 'love', 'connection', 'between', 'molly', 'bolt', 'wasn', 'necessary', 'plot', 'found', 'life', 'stinks', 'mel', 'brooks', 'observant', 'films', 'where', 'prior', 'being', 'comedy', 'shows', 'tender', 'side', 'compared', 'slapstick', 'work', 'such', 'blazing', 'saddles', 'young', 'frankenstein', 'spaceballs', 'matter', 'show', 'having', 'something', 'valuable', 'before', 'losing', 'next', 'day', 'other', 'hand', 'making', 'stupid', 'bet', 'rich', 'people', 'do', 'don', 'know', 'do', 'their', 'money', 'maybe', 'should', 'give', 'homeless', 'instead', 'using', 'monopoly', 'money', 'maybe', 'will', 'inspire', 'help', 'others']\n"
     ]
    }
   ],
   "source": [
    "# Define top common 50 words as stop words\n",
    "stop_words = {*()}\n",
    "\n",
    "for c in wordCounter.most_common(50):\n",
    "    stop_words.add(c[0])\n",
    "\n",
    "# Drop stop words\n",
    "documents = []\n",
    "\n",
    "for i in range(0, len(comments)):\n",
    "    documents.append([ w for w in comments[i] if w not in stop_words ])\n",
    "\n",
    "# Confirm the stop words are discarded\n",
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuned Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-23 17:05:29,856 : INFO : collecting all words and their counts\n",
      "2018-08-23 17:05:29,857 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-08-23 17:05:30,130 : INFO : PROGRESS: at sentence #10000, processed 1334470 words, keeping 49341 word types\n",
      "2018-08-23 17:05:30,388 : INFO : PROGRESS: at sentence #20000, processed 2634865 words, keeping 66768 word types\n",
      "2018-08-23 17:05:30,653 : INFO : PROGRESS: at sentence #30000, processed 3956185 words, keeping 79663 word types\n",
      "2018-08-23 17:05:30,937 : INFO : PROGRESS: at sentence #40000, processed 5306842 words, keeping 90628 word types\n",
      "2018-08-23 17:05:31,217 : INFO : PROGRESS: at sentence #50000, processed 6631278 words, keeping 100400 word types\n",
      "2018-08-23 17:05:31,487 : INFO : PROGRESS: at sentence #60000, processed 7929119 words, keeping 108875 word types\n",
      "2018-08-23 17:05:31,771 : INFO : PROGRESS: at sentence #70000, processed 9251238 words, keeping 117007 word types\n",
      "2018-08-23 17:05:31,915 : INFO : collected 120879 word types from a corpus of 9908215 raw words and 75000 sentences\n",
      "2018-08-23 17:05:31,915 : INFO : Loading a fresh vocabulary\n",
      "2018-08-23 17:05:32,061 : INFO : min_count=2 retains 74203 unique words (61% of original 120879, drops 46676)\n",
      "2018-08-23 17:05:32,062 : INFO : min_count=2 leaves 9861539 word corpus (99% of original 9908215, drops 46676)\n",
      "2018-08-23 17:05:32,280 : INFO : deleting the raw counts dictionary of 120879 items\n",
      "2018-08-23 17:05:32,284 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2018-08-23 17:05:32,284 : INFO : downsampling leaves estimated 9712971 word corpus (98.5% of prior 9861539)\n",
      "2018-08-23 17:05:32,555 : INFO : estimated required memory for 74203 words and 200 dimensions: 155826300 bytes\n",
      "2018-08-23 17:05:32,556 : INFO : resetting layer weights\n",
      "2018-08-23 17:05:33,430 : INFO : training model with 10 workers on 74203 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=12\n",
      "2018-08-23 17:05:34,465 : INFO : EPOCH 1 - PROGRESS: at 9.92% examples, 942444 words/s, in_qsize 20, out_qsize 0\n",
      "2018-08-23 17:05:35,490 : INFO : EPOCH 1 - PROGRESS: at 21.03% examples, 996355 words/s, in_qsize 17, out_qsize 2\n",
      "2018-08-23 17:05:36,491 : INFO : EPOCH 1 - PROGRESS: at 31.56% examples, 1003408 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:05:37,516 : INFO : EPOCH 1 - PROGRESS: at 39.65% examples, 941902 words/s, in_qsize 17, out_qsize 2\n",
      "2018-08-23 17:05:38,522 : INFO : EPOCH 1 - PROGRESS: at 47.67% examples, 913856 words/s, in_qsize 20, out_qsize 1\n",
      "2018-08-23 17:05:39,538 : INFO : EPOCH 1 - PROGRESS: at 56.81% examples, 908068 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:05:40,540 : INFO : EPOCH 1 - PROGRESS: at 67.07% examples, 920593 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:05:41,550 : INFO : EPOCH 1 - PROGRESS: at 76.73% examples, 918373 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:05:42,568 : INFO : EPOCH 1 - PROGRESS: at 86.93% examples, 925478 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:05:43,572 : INFO : EPOCH 1 - PROGRESS: at 96.09% examples, 920959 words/s, in_qsize 20, out_qsize 1\n",
      "2018-08-23 17:05:43,838 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-23 17:05:43,839 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-23 17:05:43,841 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-23 17:05:43,849 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-23 17:05:43,862 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-23 17:05:43,865 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-23 17:05:43,875 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-23 17:05:43,891 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-23 17:05:43,894 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-23 17:05:43,908 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-23 17:05:43,909 : INFO : EPOCH - 1 : training on 9908215 raw words (9713199 effective words) took 10.5s, 927421 effective words/s\n",
      "2018-08-23 17:05:44,934 : INFO : EPOCH 2 - PROGRESS: at 8.68% examples, 836811 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:05:45,936 : INFO : EPOCH 2 - PROGRESS: at 18.90% examples, 912013 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:05:46,958 : INFO : EPOCH 2 - PROGRESS: at 30.08% examples, 959457 words/s, in_qsize 17, out_qsize 2\n",
      "2018-08-23 17:05:47,975 : INFO : EPOCH 2 - PROGRESS: at 41.63% examples, 994100 words/s, in_qsize 20, out_qsize 1\n",
      "2018-08-23 17:05:48,997 : INFO : EPOCH 2 - PROGRESS: at 52.73% examples, 1011898 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:05:50,010 : INFO : EPOCH 2 - PROGRESS: at 64.08% examples, 1025384 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:05:51,025 : INFO : EPOCH 2 - PROGRESS: at 75.61% examples, 1033052 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:05:52,032 : INFO : EPOCH 2 - PROGRESS: at 86.64% examples, 1037667 words/s, in_qsize 20, out_qsize 1\n",
      "2018-08-23 17:05:53,040 : INFO : EPOCH 2 - PROGRESS: at 98.10% examples, 1044297 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:05:53,135 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-23 17:05:53,148 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-23 17:05:53,158 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-23 17:05:53,167 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-23 17:05:53,181 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-23 17:05:53,192 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-23 17:05:53,203 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-23 17:05:53,206 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-23 17:05:53,207 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-23 17:05:53,218 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-23 17:05:53,219 : INFO : EPOCH - 2 : training on 9908215 raw words (9712910 effective words) took 9.3s, 1043929 effective words/s\n",
      "2018-08-23 17:05:54,236 : INFO : EPOCH 3 - PROGRESS: at 10.56% examples, 1024468 words/s, in_qsize 17, out_qsize 2\n",
      "2018-08-23 17:05:55,248 : INFO : EPOCH 3 - PROGRESS: at 22.07% examples, 1059051 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:05:56,252 : INFO : EPOCH 3 - PROGRESS: at 33.31% examples, 1066709 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:05:57,264 : INFO : EPOCH 3 - PROGRESS: at 43.98% examples, 1056839 words/s, in_qsize 16, out_qsize 3\n",
      "2018-08-23 17:05:58,298 : INFO : EPOCH 3 - PROGRESS: at 55.30% examples, 1063303 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:05:59,310 : INFO : EPOCH 3 - PROGRESS: at 66.14% examples, 1060278 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:00,317 : INFO : EPOCH 3 - PROGRESS: at 77.64% examples, 1062949 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:01,322 : INFO : EPOCH 3 - PROGRESS: at 88.88% examples, 1067570 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:02,198 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-23 17:06:02,227 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-23 17:06:02,240 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-23 17:06:02,244 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-23 17:06:02,246 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-23 17:06:02,272 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-23 17:06:02,277 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-23 17:06:02,281 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-23 17:06:02,282 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-23 17:06:02,283 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-23 17:06:02,284 : INFO : EPOCH - 3 : training on 9908215 raw words (9713046 effective words) took 9.1s, 1072001 effective words/s\n",
      "2018-08-23 17:06:03,294 : INFO : EPOCH 4 - PROGRESS: at 10.81% examples, 1052433 words/s, in_qsize 18, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-23 17:06:04,297 : INFO : EPOCH 4 - PROGRESS: at 22.28% examples, 1078216 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:05,309 : INFO : EPOCH 4 - PROGRESS: at 33.61% examples, 1079614 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:06:06,315 : INFO : EPOCH 4 - PROGRESS: at 44.78% examples, 1082601 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:06:07,323 : INFO : EPOCH 4 - PROGRESS: at 55.78% examples, 1081581 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:06:08,325 : INFO : EPOCH 4 - PROGRESS: at 66.67% examples, 1077266 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:09,335 : INFO : EPOCH 4 - PROGRESS: at 78.31% examples, 1079825 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:06:10,339 : INFO : EPOCH 4 - PROGRESS: at 89.45% examples, 1081344 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:11,206 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-23 17:06:11,209 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-23 17:06:11,227 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-23 17:06:11,236 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-23 17:06:11,239 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-23 17:06:11,258 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-23 17:06:11,259 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-23 17:06:11,259 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-23 17:06:11,270 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-23 17:06:11,285 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-23 17:06:11,285 : INFO : EPOCH - 4 : training on 9908215 raw words (9712719 effective words) took 9.0s, 1079749 effective words/s\n",
      "2018-08-23 17:06:12,295 : INFO : EPOCH 5 - PROGRESS: at 10.78% examples, 1050969 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:13,309 : INFO : EPOCH 5 - PROGRESS: at 22.17% examples, 1066474 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:14,317 : INFO : EPOCH 5 - PROGRESS: at 33.72% examples, 1079722 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:15,318 : INFO : EPOCH 5 - PROGRESS: at 44.63% examples, 1076624 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:16,326 : INFO : EPOCH 5 - PROGRESS: at 55.28% examples, 1071205 words/s, in_qsize 17, out_qsize 2\n",
      "2018-08-23 17:06:17,348 : INFO : EPOCH 5 - PROGRESS: at 65.54% examples, 1055514 words/s, in_qsize 20, out_qsize 1\n",
      "2018-08-23 17:06:18,362 : INFO : EPOCH 5 - PROGRESS: at 75.20% examples, 1033058 words/s, in_qsize 17, out_qsize 2\n",
      "2018-08-23 17:06:19,367 : INFO : EPOCH 5 - PROGRESS: at 83.85% examples, 1009010 words/s, in_qsize 19, out_qsize 1\n",
      "2018-08-23 17:06:20,374 : INFO : EPOCH 5 - PROGRESS: at 93.87% examples, 1003966 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:06:20,826 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-23 17:06:20,870 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-23 17:06:20,871 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-23 17:06:20,876 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-23 17:06:20,879 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-23 17:06:20,905 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-23 17:06:20,913 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-23 17:06:20,920 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-23 17:06:20,923 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-23 17:06:20,924 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-23 17:06:20,925 : INFO : EPOCH - 5 : training on 9908215 raw words (9712452 effective words) took 9.6s, 1008028 effective words/s\n",
      "2018-08-23 17:06:20,926 : INFO : training on a 49541075 raw words (48564326 effective words) took 47.5s, 1022511 effective words/s\n",
      "2018-08-23 17:06:20,927 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2018-08-23 17:06:20,928 : INFO : training model with 10 workers on 74203 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=12\n",
      "2018-08-23 17:06:21,946 : INFO : EPOCH 1 - PROGRESS: at 10.49% examples, 1021207 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:22,947 : INFO : EPOCH 1 - PROGRESS: at 21.79% examples, 1053306 words/s, in_qsize 20, out_qsize 0\n",
      "2018-08-23 17:06:23,952 : INFO : EPOCH 1 - PROGRESS: at 32.69% examples, 1053070 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:06:24,986 : INFO : EPOCH 1 - PROGRESS: at 43.63% examples, 1047975 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:25,996 : INFO : EPOCH 1 - PROGRESS: at 52.83% examples, 1019180 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:06:27,022 : INFO : EPOCH 1 - PROGRESS: at 62.01% examples, 994090 words/s, in_qsize 19, out_qsize 2\n",
      "2018-08-23 17:06:28,038 : INFO : EPOCH 1 - PROGRESS: at 72.85% examples, 997963 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:29,045 : INFO : EPOCH 1 - PROGRESS: at 83.24% examples, 998418 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:30,046 : INFO : EPOCH 1 - PROGRESS: at 93.97% examples, 1002666 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:30,539 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-23 17:06:30,545 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-23 17:06:30,546 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-23 17:06:30,554 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-23 17:06:30,581 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-23 17:06:30,586 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-23 17:06:30,598 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-23 17:06:30,600 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-23 17:06:30,600 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-23 17:06:30,602 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-23 17:06:30,603 : INFO : EPOCH - 1 : training on 9908215 raw words (9713680 effective words) took 9.7s, 1005182 effective words/s\n",
      "2018-08-23 17:06:31,618 : INFO : EPOCH 2 - PROGRESS: at 10.18% examples, 988989 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:32,625 : INFO : EPOCH 2 - PROGRESS: at 20.41% examples, 985651 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:33,646 : INFO : EPOCH 2 - PROGRESS: at 29.12% examples, 928910 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:34,679 : INFO : EPOCH 2 - PROGRESS: at 38.60% examples, 917511 words/s, in_qsize 19, out_qsize 1\n",
      "2018-08-23 17:06:35,681 : INFO : EPOCH 2 - PROGRESS: at 48.87% examples, 938962 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:36,712 : INFO : EPOCH 2 - PROGRESS: at 59.34% examples, 947474 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:37,721 : INFO : EPOCH 2 - PROGRESS: at 64.87% examples, 889391 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:38,731 : INFO : EPOCH 2 - PROGRESS: at 72.28% examples, 864724 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:39,754 : INFO : EPOCH 2 - PROGRESS: at 78.58% examples, 834884 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:06:40,789 : INFO : EPOCH 2 - PROGRESS: at 86.06% examples, 821559 words/s, in_qsize 17, out_qsize 2\n",
      "2018-08-23 17:06:41,799 : INFO : EPOCH 2 - PROGRESS: at 95.87% examples, 832373 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:42,117 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-23 17:06:42,119 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-23 17:06:42,124 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-23 17:06:42,136 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-23 17:06:42,140 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-23 17:06:42,145 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-23 17:06:42,170 : INFO : worker thread finished; awaiting finish of 3 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-23 17:06:42,174 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-23 17:06:42,182 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-23 17:06:42,186 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-23 17:06:42,186 : INFO : EPOCH - 2 : training on 9908215 raw words (9712748 effective words) took 11.6s, 838859 effective words/s\n",
      "2018-08-23 17:06:43,206 : INFO : EPOCH 3 - PROGRESS: at 8.27% examples, 803025 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:44,247 : INFO : EPOCH 3 - PROGRESS: at 17.83% examples, 849667 words/s, in_qsize 16, out_qsize 3\n",
      "2018-08-23 17:06:45,271 : INFO : EPOCH 3 - PROGRESS: at 29.01% examples, 913539 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:06:46,291 : INFO : EPOCH 3 - PROGRESS: at 40.54% examples, 958752 words/s, in_qsize 17, out_qsize 2\n",
      "2018-08-23 17:06:47,300 : INFO : EPOCH 3 - PROGRESS: at 51.32% examples, 980280 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:06:48,300 : INFO : EPOCH 3 - PROGRESS: at 62.62% examples, 999371 words/s, in_qsize 17, out_qsize 2\n",
      "2018-08-23 17:06:49,305 : INFO : EPOCH 3 - PROGRESS: at 72.44% examples, 990342 words/s, in_qsize 19, out_qsize 1\n",
      "2018-08-23 17:06:50,319 : INFO : EPOCH 3 - PROGRESS: at 80.77% examples, 965817 words/s, in_qsize 17, out_qsize 2\n",
      "2018-08-23 17:06:51,370 : INFO : EPOCH 3 - PROGRESS: at 91.37% examples, 968412 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:52,135 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-23 17:06:52,137 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-23 17:06:52,146 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-23 17:06:52,148 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-23 17:06:52,160 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-23 17:06:52,162 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-23 17:06:52,169 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-23 17:06:52,181 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-23 17:06:52,183 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-23 17:06:52,187 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-23 17:06:52,188 : INFO : EPOCH - 3 : training on 9908215 raw words (9712805 effective words) took 10.0s, 971673 effective words/s\n",
      "2018-08-23 17:06:53,222 : INFO : EPOCH 4 - PROGRESS: at 10.18% examples, 972028 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:06:54,254 : INFO : EPOCH 4 - PROGRESS: at 21.12% examples, 998657 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:55,264 : INFO : EPOCH 4 - PROGRESS: at 31.96% examples, 1011115 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:56,275 : INFO : EPOCH 4 - PROGRESS: at 42.51% examples, 1010536 words/s, in_qsize 16, out_qsize 3\n",
      "2018-08-23 17:06:57,292 : INFO : EPOCH 4 - PROGRESS: at 52.92% examples, 1012792 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:58,299 : INFO : EPOCH 4 - PROGRESS: at 63.50% examples, 1014170 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:06:59,308 : INFO : EPOCH 4 - PROGRESS: at 74.62% examples, 1018958 words/s, in_qsize 17, out_qsize 2\n",
      "2018-08-23 17:07:00,344 : INFO : EPOCH 4 - PROGRESS: at 84.18% examples, 1003725 words/s, in_qsize 17, out_qsize 2\n",
      "2018-08-23 17:07:01,372 : INFO : EPOCH 4 - PROGRESS: at 94.19% examples, 996999 words/s, in_qsize 20, out_qsize 0\n",
      "2018-08-23 17:07:01,813 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-23 17:07:01,820 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-23 17:07:01,838 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-23 17:07:01,850 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-23 17:07:01,865 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-23 17:07:01,870 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-23 17:07:01,872 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-23 17:07:01,880 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-23 17:07:01,884 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-23 17:07:01,885 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-23 17:07:01,887 : INFO : EPOCH - 4 : training on 9908215 raw words (9713416 effective words) took 9.7s, 1002072 effective words/s\n",
      "2018-08-23 17:07:02,894 : INFO : EPOCH 5 - PROGRESS: at 10.81% examples, 1055675 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:03,900 : INFO : EPOCH 5 - PROGRESS: at 21.35% examples, 1034134 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:04,940 : INFO : EPOCH 5 - PROGRESS: at 30.66% examples, 977172 words/s, in_qsize 20, out_qsize 1\n",
      "2018-08-23 17:07:05,999 : INFO : EPOCH 5 - PROGRESS: at 37.45% examples, 881095 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:07:07,013 : INFO : EPOCH 5 - PROGRESS: at 43.58% examples, 826158 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:08,031 : INFO : EPOCH 5 - PROGRESS: at 49.56% examples, 787149 words/s, in_qsize 17, out_qsize 2\n",
      "2018-08-23 17:07:09,033 : INFO : EPOCH 5 - PROGRESS: at 57.69% examples, 788171 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:10,039 : INFO : EPOCH 5 - PROGRESS: at 67.85% examples, 812223 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:07:11,046 : INFO : EPOCH 5 - PROGRESS: at 78.59% examples, 834275 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:12,058 : INFO : EPOCH 5 - PROGRESS: at 88.87% examples, 850515 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:12,959 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-23 17:07:12,969 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-23 17:07:12,979 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-23 17:07:12,992 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-23 17:07:13,000 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-23 17:07:13,003 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-23 17:07:13,008 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-23 17:07:13,018 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-23 17:07:13,020 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-23 17:07:13,026 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-23 17:07:13,026 : INFO : EPOCH - 5 : training on 9908215 raw words (9712612 effective words) took 11.1s, 872366 effective words/s\n",
      "2018-08-23 17:07:14,059 : INFO : EPOCH 6 - PROGRESS: at 10.17% examples, 973383 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:07:15,097 : INFO : EPOCH 6 - PROGRESS: at 21.02% examples, 991737 words/s, in_qsize 16, out_qsize 3\n",
      "2018-08-23 17:07:16,104 : INFO : EPOCH 6 - PROGRESS: at 30.28% examples, 957028 words/s, in_qsize 20, out_qsize 2\n",
      "2018-08-23 17:07:17,106 : INFO : EPOCH 6 - PROGRESS: at 38.62% examples, 917117 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:07:18,110 : INFO : EPOCH 6 - PROGRESS: at 49.24% examples, 945955 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:19,112 : INFO : EPOCH 6 - PROGRESS: at 60.21% examples, 964205 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:07:20,129 : INFO : EPOCH 6 - PROGRESS: at 71.34% examples, 977690 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:07:21,156 : INFO : EPOCH 6 - PROGRESS: at 82.77% examples, 990178 words/s, in_qsize 16, out_qsize 3\n",
      "2018-08-23 17:07:22,161 : INFO : EPOCH 6 - PROGRESS: at 94.19% examples, 1002377 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:07:22,572 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-23 17:07:22,586 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-23 17:07:22,594 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-23 17:07:22,598 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-23 17:07:22,601 : INFO : worker thread finished; awaiting finish of 5 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-23 17:07:22,608 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-23 17:07:22,616 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-23 17:07:22,628 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-23 17:07:22,629 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-23 17:07:22,632 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-23 17:07:22,633 : INFO : EPOCH - 6 : training on 9908215 raw words (9712217 effective words) took 9.6s, 1011738 effective words/s\n",
      "2018-08-23 17:07:23,643 : INFO : EPOCH 7 - PROGRESS: at 10.90% examples, 1059928 words/s, in_qsize 20, out_qsize 1\n",
      "2018-08-23 17:07:24,652 : INFO : EPOCH 7 - PROGRESS: at 22.38% examples, 1078306 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:25,657 : INFO : EPOCH 7 - PROGRESS: at 33.42% examples, 1072881 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:07:26,665 : INFO : EPOCH 7 - PROGRESS: at 44.71% examples, 1079183 words/s, in_qsize 17, out_qsize 2\n",
      "2018-08-23 17:07:27,666 : INFO : EPOCH 7 - PROGRESS: at 55.85% examples, 1084556 words/s, in_qsize 20, out_qsize 0\n",
      "2018-08-23 17:07:28,675 : INFO : EPOCH 7 - PROGRESS: at 67.17% examples, 1084678 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:07:29,678 : INFO : EPOCH 7 - PROGRESS: at 74.81% examples, 1032309 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:07:30,689 : INFO : EPOCH 7 - PROGRESS: at 82.25% examples, 993024 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:31,700 : INFO : EPOCH 7 - PROGRESS: at 89.45% examples, 960420 words/s, in_qsize 17, out_qsize 2\n",
      "2018-08-23 17:07:32,712 : INFO : EPOCH 7 - PROGRESS: at 96.87% examples, 934207 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:32,918 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-23 17:07:32,919 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-23 17:07:32,921 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-23 17:07:32,937 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-23 17:07:32,940 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-23 17:07:32,960 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-23 17:07:32,962 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-23 17:07:32,965 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-23 17:07:32,965 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-23 17:07:32,981 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-23 17:07:32,981 : INFO : EPOCH - 7 : training on 9908215 raw words (9712916 effective words) took 10.3s, 938929 effective words/s\n",
      "2018-08-23 17:07:34,010 : INFO : EPOCH 8 - PROGRESS: at 10.27% examples, 985066 words/s, in_qsize 17, out_qsize 2\n",
      "2018-08-23 17:07:35,011 : INFO : EPOCH 8 - PROGRESS: at 21.46% examples, 1030725 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:36,016 : INFO : EPOCH 8 - PROGRESS: at 31.76% examples, 1018168 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:07:37,034 : INFO : EPOCH 8 - PROGRESS: at 42.49% examples, 1019057 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:38,049 : INFO : EPOCH 8 - PROGRESS: at 53.29% examples, 1027419 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:39,069 : INFO : EPOCH 8 - PROGRESS: at 64.77% examples, 1038608 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:07:40,077 : INFO : EPOCH 8 - PROGRESS: at 75.00% examples, 1027764 words/s, in_qsize 16, out_qsize 3\n",
      "2018-08-23 17:07:41,087 : INFO : EPOCH 8 - PROGRESS: at 84.02% examples, 1008548 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:42,100 : INFO : EPOCH 8 - PROGRESS: at 92.42% examples, 985911 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:42,732 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-23 17:07:42,756 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-23 17:07:42,773 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-23 17:07:42,774 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-23 17:07:42,778 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-23 17:07:42,783 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-23 17:07:42,787 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-23 17:07:42,793 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-23 17:07:42,804 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-23 17:07:42,805 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-23 17:07:42,805 : INFO : EPOCH - 8 : training on 9908215 raw words (9712870 effective words) took 9.8s, 989220 effective words/s\n",
      "2018-08-23 17:07:43,824 : INFO : EPOCH 9 - PROGRESS: at 10.11% examples, 977402 words/s, in_qsize 19, out_qsize 2\n",
      "2018-08-23 17:07:44,829 : INFO : EPOCH 9 - PROGRESS: at 18.17% examples, 880296 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:07:45,832 : INFO : EPOCH 9 - PROGRESS: at 27.44% examples, 880335 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:46,838 : INFO : EPOCH 9 - PROGRESS: at 36.58% examples, 879537 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:47,841 : INFO : EPOCH 9 - PROGRESS: at 46.27% examples, 895221 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:48,859 : INFO : EPOCH 9 - PROGRESS: at 55.18% examples, 890711 words/s, in_qsize 20, out_qsize 2\n",
      "2018-08-23 17:07:49,862 : INFO : EPOCH 9 - PROGRESS: at 65.65% examples, 908459 words/s, in_qsize 20, out_qsize 0\n",
      "2018-08-23 17:07:50,875 : INFO : EPOCH 9 - PROGRESS: at 74.20% examples, 894251 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:51,886 : INFO : EPOCH 9 - PROGRESS: at 83.53% examples, 895031 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:52,890 : INFO : EPOCH 9 - PROGRESS: at 93.04% examples, 897439 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:07:53,445 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-23 17:07:53,452 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2018-08-23 17:07:53,467 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-23 17:07:53,475 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-23 17:07:53,488 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-23 17:07:53,506 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-23 17:07:53,509 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-23 17:07:53,514 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-23 17:07:53,528 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-23 17:07:53,529 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-23 17:07:53,529 : INFO : EPOCH - 9 : training on 9908215 raw words (9713211 effective words) took 10.7s, 906330 effective words/s\n",
      "2018-08-23 17:07:54,542 : INFO : EPOCH 10 - PROGRESS: at 10.01% examples, 971724 words/s, in_qsize 20, out_qsize 1\n",
      "2018-08-23 17:07:55,549 : INFO : EPOCH 10 - PROGRESS: at 20.62% examples, 996483 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:07:56,561 : INFO : EPOCH 10 - PROGRESS: at 32.05% examples, 1028536 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:07:57,580 : INFO : EPOCH 10 - PROGRESS: at 42.58% examples, 1021578 words/s, in_qsize 17, out_qsize 2\n",
      "2018-08-23 17:07:58,607 : INFO : EPOCH 10 - PROGRESS: at 53.10% examples, 1021500 words/s, in_qsize 18, out_qsize 1\n",
      "2018-08-23 17:07:59,608 : INFO : EPOCH 10 - PROGRESS: at 63.77% examples, 1024123 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:08:00,612 : INFO : EPOCH 10 - PROGRESS: at 74.49% examples, 1022658 words/s, in_qsize 17, out_qsize 2\n",
      "2018-08-23 17:08:01,612 : INFO : EPOCH 10 - PROGRESS: at 84.74% examples, 1019748 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:08:02,614 : INFO : EPOCH 10 - PROGRESS: at 95.28% examples, 1019479 words/s, in_qsize 19, out_qsize 0\n",
      "2018-08-23 17:08:02,970 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2018-08-23 17:08:02,979 : INFO : worker thread finished; awaiting finish of 8 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-23 17:08:02,983 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-08-23 17:08:02,985 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-08-23 17:08:02,986 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-08-23 17:08:03,014 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-08-23 17:08:03,016 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-23 17:08:03,020 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-23 17:08:03,024 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-23 17:08:03,029 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-23 17:08:03,029 : INFO : EPOCH - 10 : training on 9908215 raw words (9712597 effective words) took 9.5s, 1022861 effective words/s\n",
      "2018-08-23 17:08:03,030 : INFO : training on a 99082150 raw words (97129072 effective words) took 102.1s, 951297 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(97129072, 99082150)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train word2vec model\n",
    "word2vec = Word2Vec(documents, size=200, window=12, min_count=2, sg=0, cbow_mean=1, hs=0, negative=5, workers=10)\n",
    "word2vec.train(documents, total_examples=len(documents), epochs=10)\n",
    "\n",
    "# Uncomment to save the trained word2vec model\n",
    "#word2vec.save('saved/cbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load from Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-03 13:29:06,421 : INFO : loading Word2Vec object from saved/cbow\n",
      "2018-09-03 13:29:07,047 : INFO : loading wv recursively from saved/cbow.wv.* with mmap=None\n",
      "2018-09-03 13:29:07,047 : INFO : loading vectors from saved/cbow.wv.vectors.npy with mmap=None\n",
      "2018-09-03 13:29:07,071 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-09-03 13:29:07,072 : INFO : loading vocabulary recursively from saved/cbow.vocabulary.* with mmap=None\n",
      "2018-09-03 13:29:07,072 : INFO : loading trainables recursively from saved/cbow.trainables.* with mmap=None\n",
      "2018-09-03 13:29:07,073 : INFO : loading syn1neg from saved/cbow.trainables.syn1neg.npy with mmap=None\n",
      "2018-09-03 13:29:07,097 : INFO : setting ignored attribute cum_table to None\n",
      "2018-09-03 13:29:07,098 : INFO : loaded saved/cbow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 74203\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load previously saved word2vec model\n",
    "word2vec = Word2Vec.load('saved/cbow')\n",
    "\n",
    "print(\"Vocabulary size: {}\".format(len(word2vec.wv.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8038068576153075\n",
      "0.4181480684981068\n",
      "0.7529960203405847\n",
      "0.740081930496738\n",
      "0.683098283886495\n",
      "0.24792129284983383\n"
     ]
    }
   ],
   "source": [
    "print(word2vec.wv.similarity('favorite', 'favorites'))\n",
    "print(word2vec.wv.similarity('awesome', 'wonderful'))\n",
    "print(word2vec.wv.similarity('incredible', 'amazing'))\n",
    "print(word2vec.wv.similarity('stunning', 'breathtaking'))\n",
    "print(word2vec.wv.similarity('bad', 'terrible'))\n",
    "print(word2vec.wv.similarity('unbearable', 'unacceptable'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2599207420321007\n",
      "0.10009993877336461\n",
      "0.200659301012579\n",
      "0.24772303689030775\n",
      "0.7419782476344514\n",
      "0.1622880533422858\n"
     ]
    }
   ],
   "source": [
    "print(word2vec.wv.similarity('amazing', 'sucks'))\n",
    "print(word2vec.wv.similarity('astounding', 'aweful'))\n",
    "print(word2vec.wv.similarity('unbelievable', 'unimpressive'))\n",
    "print(word2vec.wv.similarity('excellent', 'poor'))\n",
    "print(word2vec.wv.similarity('positive', 'negative'))\n",
    "print(word2vec.wv.similarity('dirty', 'clean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-28 13:18:58,076 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('recommended', 0.7928755283355713),\n",
       " ('advise', 0.7442986965179443),\n",
       " ('suggest', 0.7286475896835327),\n",
       " ('reccomend', 0.7021902203559875),\n",
       " ('unrecommended', 0.6527540683746338),\n",
       " ('recomend', 0.632939338684082),\n",
       " ('recommendable', 0.5861150026321411),\n",
       " ('reccomended', 0.5715225338935852)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar(positive='recommend', topn=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sucked', 0.6135208010673523),\n",
       " ('suck', 0.5909332036972046),\n",
       " ('stinks', 0.5772475004196167),\n",
       " ('crappy', 0.5602458715438843),\n",
       " ('horrible', 0.5561507344245911),\n",
       " ('terrible', 0.5478410124778748),\n",
       " ('bad', 0.5452572703361511),\n",
       " ('awful', 0.5393137335777283)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar(positive='sucks', topn=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.5108752 ,  1.337005  , -0.437047  , -1.7480597 ,  1.1979775 ,\n",
       "        1.6323268 ,  0.00981928,  1.2754765 , -1.2449191 ,  0.22516772,\n",
       "        0.12010574,  1.0527074 ,  1.5682627 , -1.3688421 ,  0.90340936,\n",
       "       -2.600385  ,  1.5435085 , -0.5717993 , -0.48193264, -0.04246677,\n",
       "        2.7148178 ,  0.14691818, -2.071621  , -1.6153599 , -0.26190206,\n",
       "       -1.5813088 ,  0.9991545 ,  0.30727467,  1.009813  , -1.3477129 ,\n",
       "        0.6699833 , -0.5330103 , -0.6237116 , -0.53601646, -2.8532028 ,\n",
       "       -0.8775494 , -0.10150794,  1.2092814 ,  1.4269018 , -0.32772237,\n",
       "       -0.9621033 , -0.67297024, -0.3645807 , -0.4485354 ,  1.372488  ,\n",
       "        0.6283808 , -0.38914105, -1.3458924 ,  1.2632807 ,  0.16047363,\n",
       "        0.105783  ,  0.22118898,  1.8390263 , -1.4369751 , -0.4344334 ,\n",
       "       -0.6878494 , -1.1227077 ,  0.15004799,  0.41653243,  0.00818901,\n",
       "        0.49397334,  0.18304007, -0.41199353,  1.438273  ,  1.2397071 ,\n",
       "       -0.9517839 , -0.9817719 ,  1.076403  ,  0.14865848,  1.7915013 ,\n",
       "       -0.40921506,  1.8432295 , -2.9876292 , -0.78627694, -0.22911915,\n",
       "       -1.1846067 ,  1.0556098 ,  0.7954007 , -0.06803293,  3.1834893 ,\n",
       "       -2.2304015 , -2.1500597 , -1.5337585 , -0.25881225, -0.61694527,\n",
       "       -1.2388973 , -0.2704042 ,  0.7864215 , -1.9451824 , -0.26300472,\n",
       "       -0.12907243,  1.4831073 ,  2.4224956 , -1.259814  , -0.42058328,\n",
       "       -2.0082974 , -1.9814628 , -1.3885573 , -2.2403142 , -0.3552228 ,\n",
       "       -0.66747904,  2.184037  ,  0.9860249 ,  0.08669426, -1.244451  ,\n",
       "       -1.4687487 , -0.78817147,  1.9542838 , -1.2840859 ,  1.0309128 ,\n",
       "       -0.73886484,  3.1052563 ,  0.46464378, -0.8706395 ,  0.90051925,\n",
       "       -0.6422655 ,  1.5000275 , -0.8524777 ,  0.7914339 , -0.7349095 ,\n",
       "        2.0068815 , -0.97696006, -0.4396973 ,  1.0067867 , -0.3803606 ,\n",
       "       -0.54686123, -0.02458439,  0.44745114,  2.4231005 ,  1.1736352 ,\n",
       "        1.4051843 , -1.0017354 ,  0.2775823 ,  1.2981355 ,  2.0056443 ,\n",
       "       -0.82041085,  0.28725109,  1.8318851 , -0.07333085, -0.86985004,\n",
       "       -1.2415652 ,  1.064072  , -0.23426841,  0.6402331 ,  3.1774638 ,\n",
       "       -0.36066112, -1.430772  , -0.02688587, -0.8211965 ,  0.24387465,\n",
       "        0.9698117 ,  0.17454055, -0.5025765 ,  2.0355186 ,  1.0204232 ,\n",
       "        1.0851866 ,  1.0092095 , -0.6684947 ,  0.1625443 ,  0.60430074,\n",
       "        1.1896738 , -0.58610183,  2.5152707 , -1.012467  ,  1.5535563 ,\n",
       "        0.937469  , -0.560587  ,  0.53786653, -0.9127748 , -3.5968997 ,\n",
       "        0.78640527,  1.2308103 , -0.99261487,  1.2441475 , -0.2598358 ,\n",
       "       -0.6641645 , -0.7253134 ,  1.4535855 ,  0.06533956,  1.4537672 ,\n",
       "       -1.4275286 ,  0.7273938 ,  1.4068215 , -1.1128802 ,  0.25364265,\n",
       "       -2.8133457 ,  1.4574202 , -0.8248527 , -1.0849515 , -1.0229586 ,\n",
       "       -2.1216054 , -1.6331073 , -0.8485882 , -1.2516254 , -0.13928618,\n",
       "       -1.5687153 ,  0.31218332,  1.2957492 ,  2.6303542 , -0.2785485 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.get_vector('incredible')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split from Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the trainset into 80% for training and 20% for validating\n",
    "vectorizer = MeanWordVectorizer(word2vec)\n",
    "X1 = vectorizer.transform(X_train)\n",
    "X2 = vectorizer.transform(X_test)\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Word2Vec + Default Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.60%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_train1, y_train1)\n",
    "y_pred = logistic.predict(X_test1)\n",
    "accuracy = accuracy_score(y_test1, y_pred)\n",
    "\n",
    "print(\"Test Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Word2Vec + Default XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 83.22%\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "xgboost = XGBClassifier()\n",
    "xgboost.fit(X_train1, y_train1)\n",
    "y_pred = xgboost.predict(X_test1)\n",
    "accuracy = accuracy_score(y_test1, y_pred)\n",
    "\n",
    "print(\"Test Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.52%\n",
      "Best Parameters: LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "parameters = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.25, 0.5, 0.75, 1, 2]\n",
    "}\n",
    "\n",
    "# Run grid search with 5-fold cross validation\n",
    "clf = GridSearchCV(logistic, parameters, cv=5, verbose=0)\n",
    "best_model = clf.fit(X_train1, y_train1)\n",
    "y_pred = best_model.predict(X_test1)\n",
    "accuracy = accuracy_score(y_test1, y_pred)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print('Best Parameters:', best_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.80%\n",
      "Best Parameters: XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.04, max_delta_step=0,\n",
      "       max_depth=6, min_child_weight=1, missing=None, n_estimators=800,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=1,\n",
      "       subsample=0.6)\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgboost = XGBClassifier()\n",
    "parameters = {'objective': ['binary:logistic'],\n",
    "              'learning_rate': [0.04, 0.05],\n",
    "              'max_depth': [6],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.6, 0.8],\n",
    "              'n_estimators': [800, 1000]}\n",
    "\n",
    "# Run grid search with 5-fold cross validation\n",
    "clf = GridSearchCV(xgboost, parameters, cv=5, verbose=0)\n",
    "best_model = clf.fit(X_train1, y_train1)\n",
    "y_pred = best_model.predict(X_test1)\n",
    "accuracy = accuracy_score(y_test1, y_pred)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print('Best Parameters:', best_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.04%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logistic = LogisticRegression(C=1, penalty='l1')\n",
    "logistic.fit(X1, y_train)\n",
    "y_pred = logistic.predict(X2)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 87.58%\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "xgboost = XGBClassifier(max_depth=6, n_estimators=800, learning_rate=0.04, objective='binary:logistic', subsample=0.6, silent=0)\n",
    "xgboost.fit(X1, y_train)\n",
    "y_pred = xgboost.predict(X2)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost Overfitting Resistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Run XGBoost classifier with increasing number of rounds\n",
    "def test_xgboost(rounds):\n",
    "    xgboost = XGBClassifier(max_depth=6, n_estimators=rounds, learning_rate=0.04, objective='binary:logistic', subsample=0.6, silent=0)\n",
    "    xgboost.fit(X1, y_train)\n",
    "    y_pred1 = xgboost.predict(X1)\n",
    "    y_pred2 = xgboost.predict(X2)\n",
    "    accuracy1 = accuracy_score(y_train, y_pred1)\n",
    "    accuracy2 = accuracy_score(y_test, y_pred2)\n",
    "    \n",
    "    print(\"Train Accuracy: %.2f%%\" % (accuracy1 * 100.0))\n",
    "    print(\"Test Accuracy: %.2f%%\" % (accuracy2 * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 90.18%\n",
      "Test Accuracy: 84.25%\n"
     ]
    }
   ],
   "source": [
    "test_xgboost(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 94.23%\n",
      "Test Accuracy: 86.07%\n"
     ]
    }
   ],
   "source": [
    "test_xgboost(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 98.83%\n",
      "Test Accuracy: 87.24%\n"
     ]
    }
   ],
   "source": [
    "test_xgboost(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 99.98%\n",
      "Test Accuracy: 87.60%\n"
     ]
    }
   ],
   "source": [
    "test_xgboost(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 100.00%\n",
      "Test Accuracy: 87.76%\n"
     ]
    }
   ],
   "source": [
    "test_xgboost(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 100.00%\n",
      "Test Accuracy: 87.87%\n"
     ]
    }
   ],
   "source": [
    "test_xgboost(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 100.00%\n",
      "Test Accuracy: 87.95%\n"
     ]
    }
   ],
   "source": [
    "test_xgboost(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 100.00%\n",
      "Test Accuracy: 87.95%\n"
     ]
    }
   ],
   "source": [
    "test_xgboost(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEXCAYAAAC3c9OwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8VOXZ//HPNwsk7IRVNkFqVURAGhXcV9xFbXGpVsQFtfVxeX5qtbV1aeujVmtdqpaq6NMluFDrigqC2OoDCAoSQGWHAGFPWJJAluv3xzmJY5gkQ5LJkOR6v17zmjn3OXPOdc8k55pzn3PuW2aGc845V1lSogNwzjm3b/IE4ZxzLipPEM4556LyBOGccy4qTxDOOeei8gThnHMuKk8Qzu0lSd0kfSxpu6RHJf1C0nN7uY5nJf0qXjHWN0l9JO2QlJzoWFzD8QTRxElqI2mFpB9HlLWVtErSjyLKMiW9LWmrpDxJCyX9TlLHcP6VkkrDncQOScsk3RDn2E+UlBPDckdLmhrusPMlvSVpQBxDGwtsAtqZ2f8zswfM7Jowlr6STFJKRHxXSvpP5ArM7Hoz+019Bxax/fLvaYWkO+u6XjNbZWZtzKy0hu3vUVfXeHmCaOLMbAfBDu1xSV3C4oeB2Wb2GgQ7WOAj4BPgYDPrAJwBlACDI1b3f+FOog3wI+BhSYc3TE2ikzQc+AB4A+gB9APmAZ9IOqCetyVJScD+wELbt+8y7RDxPf1K0mmJDsg1Qmbmj2bwAF4EsoATgc3AfhHz/gM8WcP7rwT+U6lsFvDjiOnzgAVAHkHCOSRi3iFhWV64zHkR884CFgLbgTXAbUBroBAoA3aEjx5R4vo38HSU8knA/4avFwHnRMxLITgCGBpODwM+DWObB5wYsexHwO8Ikmch8DegGNgdxnQqcC/wt3D5VYBFxDwcKAJKw+m8iO/jt+HrE4Ec4P8BG4B1wJiIGDoBbwHbgM+A31b+LiKW7RtuP6XS93R7xHQPYCKwEVgO3BQx70hgdrit9cAfoq03/HtYFn5ny4HLwu84Wl1bAo+En8164FkgPca6pwOPAiuBfIK/1fSavjd/1NN+I9EB+KOBvmjoGP7zbar0D9g6/Ic+sYb3Xxm5UwKOCP8xvx9Ofx/YCZwGpAJ3AEuAFuH0EuAX4fTJ4Y7loPC964DjIuIs33GfCORUE1OrMPaToswbA6wLX/8a+HvEvLOBr8LXPQkS5lkER9SnhdNdwvkfhTu2QwkSSyoRO/dwmXv5NkF8Z0ca7bMLyyrWEdazBLg/XP9ZQAHQMZw/IXy0AgYAqyuvL2K939l+uBMtAC4Ip5OAOeFn0gI4gGBHf3o4//+An4Sv2wDDKq83/JvZFvH97QccWk1d/wi8CWQAbQmS3f/EWPc/hd9BTyAZOJog4VT7vfmjfh7exNRMmNlWgl/urYB/RszqSPAPllteIOnh8DzETkl3Ryw7LCzfQfCr9K/A4nDexcA7ZjbZzIoJfjGmE/xDDyPY2TxoZrvNbCrwNnBp+N5iYICkdma21cw+j7FaGWHs66LMWwd0Dl//AzhPUqtw+sdhGcDlwLtm9q6ZlZnZZIJf0GdFrOtFM1tgZiVh3eKhGLjfzIrN7F2CX+AHhSeFfwjcY2YFZrYQeCmG9W2SVEiww38a+FdYfgTBTvT+8LtYBvwFuCQiju9J6mxmO8xsRhXrLwMGSko3s3VmtiDaQpIEXAvcamZbzGw78EDE9qqrexJwFXCzma0xs1Iz+9TMdhHb9+bqyBNEMyHpcoJfgVOAhyJmbSX4Z9+vvMDM7rDgPMTrBL8Yy80ws/K27e4Ev6ofCOf1IGgGKF9HGcEv3Z7hvNVhWbmV4TwIdoBnASslTQ/PK8Rij9gj7EdwtISZLSFoZjo3TBLn8W2C2B8YFSa+PEl5wLGV1rk6xnjqYrOZlURMFxAk1S4E30FkDLHE0zl8/20Ev9JTw/L9gR6V6vsLoFs4/2qCo8GvJH0m6ZzKKzaznQQ/CK4H1kl6R9LBVcTRheBHyZyI7b0XltdU985AGrA0ynpj+d5cHXmCaAYkdQUeI/gldx1wkaTjoeKffSZw4d6s08zWE7RjnxsWrSX4py3fpoDeBOcU1gK9w1+E5fqE8zCzz8xsJNCV4JfuK+WbqSGGnQS/kEdFmX0R8GHEdBbBEctIghPMS8Ly1cBfw8RX/mhtZg9Gbqq6OCqHFWNZrDYSNMH0iijrHVMgwS/uRwnOC/w0LF4NLK9U37Zmdlb4nsVmdinBd/EQ8Jqk1lHW/b6ZnUawQ/6K4CgE9qzrJoJzN4dGbK99+COjJpvC2PtHmRfL9+bqyBNE8/AU8C8zm2Zm6wjOD/xFUstw/h3AVZLuDJMJknoRXBEUlaROwAUEzVYQ7NTPlnSKpFSCk467CE4iziQ4P3GHpFRJJxIklgmSWki6TFL7sPlmG8F5BQhOaHaS1L6aut0JjJZ0U3j5bkdJvyU4OXxfxHITgBHADXx79ADBSedzJZ0uKVlSWnh5beQOeW9sJDiqibyCaj3QS1KLvV2ZBZeV/hO4V1Kr8Jf6FXu5mgcJPvs0gqbBbZJ+Lik9rPNASUdAcKQpqUt4tJcXvv87l7YquA/kvDBx7CJoEor8zirqGq7nL8BjEX9bPSWdHkPdy4AXgD9I6hHGOjz8u63v781F4QmiiZN0PsGh9+3lZWb2HMGVI78Op/9DcOL4eOCbiGaAj4AnI1Y3vPz6eoImm43Af4Xr+JqgXfhJgl9+5wLnhu3cuwmadc4M5z0NXGFmX4Xr/QmwQtI2gmaLy8N1fkXwy39Z2IzQo3L9wthPJzgCWkfQdHU4cKyZLY5Ybh3B0cbRwMsR5asJjip+EdZndfhZ1ep/w8wKCK96CmMeBkwlSKS5kjbVYrU3Au0JzhP9leAz2bUX73+HoDnu2jDhnAsMIbj6aBPwXLh+CC5vXhB+x48Dl5hZUaX1JRH8AFgLbAFO4NsjlGh1/TnBRQozwu94CnBQjLHfBswnuHprC8FRTVJ9f28uOpnty5dyO+cqk/QQ0N3MRic6Fte0ebZ1bh8n6WBJg8Ib9Y4kOJH8eqLjck1fSs2LOOcSrC1Bs1IPgpvJHiW4c9y5uPImJuecc1F5E5NzzrmoGnUTU+fOna1v376JDsM55xqVOXPmbDKzLjUt16gTRN++fZk9e3aiw3DOuUZF0sqal/ImJuecc1XwBOGccy4qTxDOOeei8gThnHMuKk8QzjnnoopbgpD0gqQNkrIjyjIkTZa0OHzuGJZL0hOSlkj6UtLQeMXlnHMuNvE8gniRoGfISHcCH5rZgQR99d8Zlp8JHBg+xgLPxDEu55xzMYjbfRBm9rGkvpWKRxKMbgXBsIkfEXQFPJJggHkj6BK4g6T9wi6a3V7aXVLGqi07KS2DMrPgEfk6HIu8zL4ts4rXUFb27XJl4di0ZRHzLXxP+forz8e7b3Eu7o7s14mDureN6zYa+ka5buU7fTNbVz6ACMHQk5HDKOaEZXskCEljCY4y6NOnT3yjbWS2FxWTNWsVz/9nOeu37c1wAc65xua35w9scgmiKopSFvVnqJmNA8YBZGZm+k9VYOP2XYz/ZDl/nbGS7UUlHN2/E3ecfjCtWiQjiSRBkkRSEuH0t2USJEskJQVl0eYnRZRFri85Kfp8Rfs2nXP1qk3L+O++GzpBrC9vOpK0H0HXxRAcMUSOs9uLYLQqV40Vm3Yy7t/LeG1ODsWlZZw5sDvXHd+fwb07JDo051wT0NAJ4k1gNMEYuaP5tk/7N4EbJU0AjgLy/fxD1b7MyePZ6UuZlJ1LanISPxzai7HHH0C/znuMLe+cc7UWtwQhKYvghHRnSTnAPQSJ4RVJVwOrgFHh4u8CZxGMW1sAjIlXXI2VmfHvxZt4dvpSPl26mbZpKVx/Qn/GHNOXrm3TEh2ec64JiudVTJdWMeuUKMsa8LN4xdKYlZSW8W52Ln+evpQFa7fRtW1L7jrzYH58VB/apqUmOjznXBO2r5ykdpUUFZfy6uzV/OXfy1m1pYADurTm4R8OYuThPWiZkpzo8JxzzYAniH1MXsFu/vp/K3nx0xVs3rmbw/t04JdnH8Jph3QjKckvD3LONRxPEPuItXmFPP+f5WTNWkXB7lJOOqgL15/QnyP7ZSC/btQ5lwCeIBJs8frtPDt9GW/MXYMB5w7aj+tO6M8h+7VLdGjOuWbOE0SCzF6xhWenL2XKog2kpyZz+bD9uea4fvTq2CrRoTnnHOAJokGVlRlTv9rAs9OXMnvlVjq2SuWWUw/kiuF9yWjdItHhOefcd3iCaAC7S8p4c95a/jx9KYs37KBnh3TuPXcAFx3Rm1Yt/Ctwzu2bfO8URzt3lVR0nrcuv4iDu7fljxcP4exB+5Ga7GM1Oef2bZ4g4mTywvXc9uo88guLOapfBg9ceBgnfr+LX5HknGs0PEHEyR+nfEOn1i14ccwRHN6nY6LDcc65vebtHHGwanMBC9Zu49Ij+3hycM41Wp4g4uC9BUFHtGcM7J7gSJxzrvY8QcTBe9m5HNqjHb0z/J4G51zj5QminuXmF/H5qjzO9KMH51wj5wminn2wMBfw5iXnXOPnCaKeTZqfy/e6tuF7XeM7mLhzzsVbQhKEpJslZUtaIOmWsGyIpBmS5kqaLenIRMRWF5t37GLm8s2ccagfPTjnGr8GTxCSBgLXAkcCg4FzJB0IPAzcZ2ZDgF+H043KlEXrKTNvXnLONQ2JuFHuEGCGmRUASJoOXAAYUN7HdXtgbQJiq5NJ2bn0zkjn0B7eVbdzrvFLRBNTNnC8pE6SWgFnAb2BW4DfS1oNPALcFe3NksaGTVCzN27c2GBB1yS/sJhPlmzijEO7e3cazrkmocEThJktAh4CJgPvAfOAEuAG4FYz6w3cCjxfxfvHmVmmmWV26dKlgaKu2bSvNlBcapwxcL9Eh+Kcc/UiISepzex5MxtqZscDW4DFwGjgn+EirxKco2g0JmWvo1u7lhzeu0OiQ3HOuXqRqKuYuobPfYALgSyCcw4nhIucTJA0GoWC3SVM/2Yjpx/anaQkb15yzjUNierNdaKkTkAx8DMz2yrpWuBxSSlAETA2QbHttelfb6SouMyvXnLONSkJSRBmdlyUsv8AP0hAOHU2KTuXjq1SObJvRqJDcc65euN3UtfRrpJSpn61gREDupPio8Q555oQ36PV0SdLNrFjVwlnHObNS865psUTRB1Nmp9L25YpHN2/U6JDcc65euUJog5KSsuYvGg9pxzSlZYpyYkOxznn6pUniDqYuXwLeQXFfnOcc65J8gRRB5Oy15GemswJ39937uh2zrn64gmilsrKjPcXrOfEg7qQ3sKbl5xzTY8niFr6fNVWNm7f5TfHOeeaLE8QtTQpO5cWyUmcfHDXRIfinHNx4QmiFsyM97JzOfbAzrRNS010OM45FxeeIGohe8021uQV+tCizrkmzRNELUzKXkdykjhtQLdEh+Kcc3HjCWIvlTcvDTsgg46tWyQ6HOecixtPEHtp8YYdLNu005uXnHNNnieIvTRpfi4SnO4JwjnXxHmC2EvvLchlaJ+OdG2XluhQnHMurhI15OjNkrIlLZB0S0T5f0n6Oix/OBGxVWfl5p0sWreNM/3mOOdcM9DgI8pJGghcCxwJ7Abek/QO0AsYCQwys13l41bvS97LzgW8eck51zwkYsjRQ4AZZlYAIGk6cAGQCTxoZrsAzGxDAmKr1qTsXAb2bEfvjFaJDsU55+IuEU1M2cDxkjpJagWcBfQGvg8cJ2mmpOmSjkhAbFVal1/I3NV5nOldezvnmokGP4Iws0WSHgImAzuAeUBJGEtHYBhwBPCKpAPMzCLfL2ksMBagT58+DRb3+2HzknfO55xrLhJyktrMnjezoWZ2PLAFWAzkAP+0wCygDOgc5b3jzCzTzDK7dGm4cRgmZedyYNc29O/SpsG26ZxziZSoq5i6hs99gAuBLOBfwMlh+feBFsCmRMRX2aYdu/hsxRa/esk516wk4iQ1wERJnYBi4GdmtlXSC8ALkrIJrm4aXbl5KVEmL1xPmcHpniCcc81IQhKEmR0XpWw3cHkCwqnRpOxc+mS0YsB+7RIdinPONRi/k7oG+QXFfLpkE2cO7I6kRIfjnHMNxhNEDT78aj0lZebNS865ZscTRA0mZefSvV0aQ3p1SHQozjnXoDxBVGPnrhI+/mYjZwzsTlKSNy8555oXTxDV+OjrjewqKfO+l5xzzdJeJQhJp0g6V1JqvALal0zKXken1i04sl9GokNxzrkGF3OCkPQocCpBVxhvxC2ifURRcSnTvtrAaQO6kezNS865ZqjK+yAkPQL8xszyw6I+wEXh6/nxDizR/rN4Ezt3l3rfS865Zqu6I4jXgZfDQXySgf8FZgBzgXENEVwiTcrOpW1aCkf336M7KOecaxaqTBBm9omZnQHkAe+FZUeZ2WAze6KhAkyE4tIypixaz6mHdKNFip/Hd841T1Xu/SSlSDobWE8woM/hkt6UNKjBokuQGcs2k19Y7M1Lzrlmrbq+mP5F0JzUCrjMzEZL6gHcL8nM7NoGiTABJmXnkp6azAnfb7juxJ1zbl9TXYLY38zOkdSC4NwDZrYWuEbSkAaJLgFKy4wPFqznpIO7kJaanOhwnHMuYapLEOMkzQUMeDRyhpnNjWtUCTRn5VY27djFGT60qHOumasyQZjZk8CTDRjLPmFS9jpaJCdx0kHevOSca978Ep0IZsb72bkcd2Bn2qY1i5vFnXOuSokacvRmSdmSFki6pdK82ySZpAa/AWHhum2szS/yrr2dc44YEkR4k1y9kTQQuBY4EhgMnCPpwHBeb+A0YFV9bjNWSzfuBGCwd+3tnHMxHUEskfR7SQPqaZuHADPMrMDMSoDpBPdZADwG3EFwYrzBrdlaCEDPjumJ2Lxzzu1TYkkQg4BvgOckzZA0VlJdBmfOBo6X1ElSK+AsoLek84A1ZjavDuuuk5ytBXRolUqblgkZqts55/YpNSYIM9tuZn8xs6MJft3fA6yT9JKk7+3tBs1sEfAQMJmgC495QAnwS+DXNb0/TFCzJc3euHHj3m6+WmvyCunZwY8enHMOYjwHIek8Sa8DjxPcE3EA8Bbwbm02ambPm9lQMzse2AKsAPoB8yStAHoBn0va42yxmY0zs0wzy+zSpX4vRV2ztZBe3rzknHNA9TfKlVsMTAN+b2afRpS/Jun42mxUUlcz2yCpD3AhMNzMHo+YvwLINLNNtVl/bZgZa/IKOe5Av//BOecgtgQxyMx2RJthZjfVcrsTJXUCioGfmdnWWq6n3mwtKKZgd6mfoHbOuVAsJ6n/JKniuk9JHSW9UJeNmtlxZjYg7Dr8wyjz+zbk0QNEXMHk5yCccw6I8SomM8srnwh/7R8ev5ASY01eAYCfg3DOuVAsCSJJUsfyCUkZxNY01ajkhEcQniCccy4Qy47+UeBTSa+F06OA38UvpMTI2VpI6xbJtE/3Ppiccw5iSBBm9r+S5gAnAQIuNLOFcY+sga3JK6RXx1ZISnQozjm3T4ipqcjMFkjaCKQBSOpjZgnpLylecrYW+hVMzjkXIZYb5c6TtBhYTtBv0gpgUpzjanBrthb4FUzOORchlpPUvwGGAd+YWT/gFOCTuEbVwLYXFbOtqMRPUDvnXIRYEkSxmW0muJopycymAU1qTOo1ed6Lq3POVRbLOYg8SW2Aj4G/S9pA0Llek5GzxW+Sc865ymI5ghgJFAC3EvS+uhQ4N55BNTQ/gnDOuT1VewQRjib3hpmdCpQBLzVIVA1sTV4hLVOS6NKmZaJDcc65fUa1RxBmVgoUSGrfQPEkRE54BZPfA+Gcc9+K5RxEETBf0mRgZ3lhHXpy3ees8XsgnHNuD7EkiHfCR5O1Jq+QAT3qMoqqc841PbF0tdEkzzuUKyouZdOO3X4Fk3POVVJjgpC0HLDK5WZ2QFwiamDlvbh6E5Nzzn1XLE1MmRGv0wh6c82oy0Yl3QxcS9D531/M7I+Sfk9w+exugktpx0SOQxEv5Ze49urYKt6bcs65RqXG+yDMbHPEY42Z/RE4ubYblDSQIDkcCQwGzpF0IDAZGGhmg4BvgLtqu4294SPJOedcdLE0MQ2NmEwiOKJoW4dtHgLMMLOCcP3TgQvM7OGIZWYAP6rDNmKWs7WAlCTRrV1aQ2zOOecajVgHDCpXQtCr60V12GY28DtJnYBC4CxgdqVlrgJersM2YrYmr5Du7dNITvJ7IJxzLlIsVzGdVJ8bNLNFkh4iaFLaAcwjom8nSb8Mp/8e7f2SxgJjAfr06VPneNZsLfReXJ1zLopYxoN4QFKHiOmOkn5bl42a2fNmNtTMjge2AIvDdY8GzgEuM7M9rpwK3zvOzDLNLLNLly51CQMIBwrq4CeonXOuslg66zsz8moiM9tK0CxUa5K6hs99gAuBLElnAD8Hzis/PxFvu0vKWL+9yC9xdc65KGI5B5EsqaWZ7QKQlA7UtVe7ieE5iGLgZ2a2VdJT4Xonh30izTCz6+u4nWrl5hdhhjcxOedcFLEkiL8BH0oaT3DD3FXUsVdXMzsuStn36rLO2sjJCw5Uevklrs45t4dYTlI/LOlL4FSCG9t+Y2bvxz2yBuB3UTvnXNViuQ+iH/CRmb0XTqdL6mtmK+IdXLyt2VqIBPu19wThnHOVxXKS+lWCwYLKlYZljd6avEK6tU2jRUosH4NzzjUvsewZU8xsd/lE+LpF/EJqODlbC7x5yTnnqhBLgtgo6bzyCUkjgU3xC6nhrMkr9D6YnHOuCrFcxXQ98PfwMlQBq4Er4hpVAygtM9blFXHuIE8QzjkXTSxXMS0FhklqA8jMtsc/rPhbv62IkjLzJibnnKtCLEcQSDobOBRIC29iw8zuj2NccVc+DoQ3MTnnXHSx9MX0LHAx8F8ETUyjgP3jHFfclY8D4QMFOedcdLGcpD7azK4AtprZfcBwoHd8w4o/P4JwzrnqxZIgCsPnAkk9CPpP6he/kBpGztYCOrVuQXqL5ESH4pxz+6RYzkG8HXb3/Xvgc4L+mP4S16gaQI6PA+Gcc9WK5Sqm34QvJ0p6G0gzs/z4hhV/a/IKObh7XUZOdc65pm2v+pgws11NITmYGWu2+k1yzjlXnWbZCdGmHbvZVVLmCcI556rRLBNE+RVMfomrc85VLZb7ID6MpawxWePjQDjnXI2qTBCS0iRlAJ0ldZSUET76Aj3qslFJN0vKlrRA0i1hWYakyZIWh88d67KN6uRsDUaS8wThnHNVq+4qpuuAWwiSwRyCu6gBtgF/qu0GJQ0ErgWOBHYD70l6Jyz70MwelHQncCfw89pupzojDu1Ot3ZptEtLjcfqnXOuSagyQZjZ48Djkv7LzJ6sx20eAswwswIASdOBC4CRwInhMi8BHxGnBNGvc2v6dW4dj1U751yTEctJ6lxJbQEk3S3pn5KG1mGb2cDxkjpJagWcRdB1RzczWwcQPneN9mZJYyXNljR748aNdQjDOedcdWJJEL8ys+2SjgVOJ/h1/0xtN2hmi4CHgMnAe8A8oGQv3j/OzDLNLLNLly61DcM551wNYkkQpeHz2cAzZvYGdRxy1MyeN7OhZnY8sAVYDKyXtB9A+LyhLttwzjlXN7EkiDWS/gxcBLwrqWWM76uSpK7hcx/gQiALeBMYHS4yGnijLttwzjlXN7F01ncRcAbwiJnlhb/ub6/jdidK6kTQM+zPzGyrpAeBVyRdDawiGHfCOedcgsTSWV+BpA3AsQRNQSXhc62Z2XFRyjYDp9Rlvc455+pPLHdS30NwueldYVEq8Ld4BuWccy7xYjmXcAFwHrATwMzWAt5PtnPONXGxJIjdZmYEAwUhye8wc865ZiCWBPFKeBVTB0nXAlOA5+IblnPOuUSL5ST1I5JOI+iD6SDg12Y2Oe6ROeecS6gaE4Skh8zs5wR3Plcuc84510TF0sR0WpSyM+s7EOecc/uWKo8gJN0A/BQ4QNKXEbPaAp/EOzDnnHOJVV0T0z+AScD/EIzNUG67mW2Ja1TOOecSrrrxIPKBfODShgvHOefcvqJOne4555xrujxBOOeci8oThHPOuag8QTjnnIvKE4RzzrmoPEE455yLKiEJQtKtkhZIypaUJSlN0imSPpc0V9J/JH0vEbE555wLNHiCkNQTuAnINLOBQDJwCfAMcJmZDSG4Se/uho7NOefct2IZkzpe202XVAy0AtYSjDfRLpzfPixzrvkyg7KSSo/S706Xhs9WmuhoXUNrux+0yojrJho8QZjZGkmPAKuAQuADM/tA0jXAu5IKCboWHxbt/ZLGAmMB+vTp00BRu31WWSkUF8DuncGjpAhKi/fckVbsXIur3tmWlYbvrWp+tEe4TMX7oqyzrCRiu9Vts9I83+m76pz9Bzji6rhuosEThKSOwEigH5AHvCrpcuBC4CwzmynpduAPwDWV329m44BxAJmZmdZggbvaM4OSXeGOfAfsLoDicIde3evdO8L3VH69M1yuAEoK4x+/kiEp5dtHcsTrpErz9ngkQ0rL8H2pUZaPnI42P1wmObX6bSoJpPh/Fm7f0X1Q3DeRiCamU4HlZrYRQNI/gWOAwWY2M1zmZeC9BMTmKjOD4kIoyoeivOC5MK/S60rzduVH7MjDnbuVxb5NJUGLNpDaClq0hhatILU1pHWAdj2C1+XlFcuFr1PSKu1Mo+xsK3bW0Xa2yRE763Dad7yumUpEglgFDJPUiqCJ6RRgNjBK0vfN7BuCMSgWJSC2pqmsFHZtq3nHXj5deV7p7urX36INpLUPduBp7aFdz2AHnhrutFu0qvp1i9bhDj9iZ5/S0nfKzu0DEnEOYqak14DPgRLgC4ImoxxgoqQyYCtwVUPH1iiVlcH2tbB5KWxZGjxvXgrbcqAwP9jB78qvfh1KDnbs6R2+3cl36P3dnX56h4jpDhHT7YNf4865JkdmjbcZPzMz02bPnp3oMOLPDHZsCBPAkohksAy2LPtuO3xKGmQcAB36QHrHanbyEdMt2vgvdueaEUlzzCyzpuUSdZmri6ZgS6UEECaELcth9/Zvl0tKhYx+kNEf+p8UJIRO34NO/aFtD0jyG+Q2VJpCAAAWcUlEQVSdc3XnCaKhFeWHCWBZRAIIk0FR3rfLKTk4CujUH/Y/OkgGnQ4Intv3Dq6kcc65OPK9TLwUboVl08MEsOzbo4KdGyMWErTvFSSBgT8MnjP6B0cDHfpASouEhe+cc54g4iFnNrxyBWxbE0y33S/Y8R905rcJoFN/6NgPUtMSG6tzzlXBE0R9m/MivHs7tO0Oo9+GHodDyzaJjso55/aaJ4j6UlwE794GX/wV+p8CP3wu7v2kOOdcPHmCqA95q+GVn8DaL+C42+CkXwR34DrnXCPmCaKulk2H18ZAyW645B9w8NmJjsg55+qFXzBfW2bwyRPw1/OhdRcYO82Tg3OuSfEjiNrYtR3euBEW/gsGjISRf4KWbRMdlXPO1StPEHtr02J4+XLY9A2cdj8cfZN3U+Gca5I8QeyNRW/D69cHN7D95HU44MRER+Scc3HjCSIWZaUw7QH49yPBfQ0X/TXo7dQ555owTxA1KdgCE6+BpR/C4T+Bsx7xu59ds1NcXExOTg5FRUWJDsXthbS0NHr16kVqau265PcEUZ1184LzDdtz4dzH4QdXJjoi5xIiJyeHtm3b0rdvX+Tn3BoFM2Pz5s3k5OTQr1+/Wq3DL3OtytwseH4ElJbAmEmeHFyzVlRURKdOnTw5NCKS6NSpU52O+hKSICTdKmmBpGxJWZLSFPidpG8kLZJ0UyJio2Q3vHMb/Ot66JkJ102HXjWOq+Fck+fJofGp63fW4E1MknoCNwEDzKxQ0ivAJYCA3sDBZlYmqWtDx8bunfDXC2H1DBh+I5x6n4+74JxrthLVxJQCpEtKAVoBa4EbgPvNrAzAzDY0eFSL3gqSw8in4fTfeXJwbh+xefNmhgwZwpAhQ+jevTs9e/asmN69e3dM6xgzZgxff/31Xm/77LPP5rjjjtvr9zUFDb4HNLM1kh4BVgGFwAdm9oGkLOBiSRcAG4GbzGxx5fdLGguMBejTp0/9Brd0KrTqDIMvrd/1OufqpFOnTsydOxeAe++9lzZt2nDbbbd9Zxkzw8xIqmLI3fHjx+/1djdv3sz8+fNJS0tj1apV9b/PCZWUlJCSsu/9IE1EE1NHYCTQD8gDXpV0OdASKDKzTEkXAi8Ae6RtMxsHjAPIzMy0egvMDJZOC25+8zGdnavSfW8tYOHabfW6zgE92nHPuYfu9fuWLFnC+eefz7HHHsvMmTN5++23ue+++/j8888pLCzk4osv5te//jUAxx57LE899RQDBw6kc+fOXH/99UyaNIlWrVrxxhtv0LXrnq3ar732Gueffz7t27fn5Zdf5vbbbwcgNzeX6667juXLlyOJcePGcdRRRzF+/Hgee+wxJDF06FDGjx/P5Zdfzo9+9CPOP/98ANq0acOOHTuYMmUKDz74IJ07d2bBggXMnz+fc889l7Vr11JUVMStt97KNddcA8A777zDr371K0pLS+nWrRuTJk3ioIMOYtasWWRkZFBaWsqBBx7I7Nmzyciov2EGErEnPBVYbmYbzawY+CdwNJADTAyXeR0Y1KBRrV8AOzdA/5MbdLPOubpZuHAhV199NV988QU9e/bkwQcfZPbs2cybN4/JkyezcOHCPd6Tn5/PCSecwLx58xg+fDgvvPBC1HVnZWVx6aWXcumll5KVlVVR/rOf/YzTTjuNL7/8kjlz5nDIIYcwb948HnroIT766CPmzZvHo48+WmPsM2bM4OGHH2b+/PkAvPTSS8yZM4fPPvuMP/zhD2zdupXc3FxuuOEGXn/9debNm8eECRNITk7m0ksv5R//+AcA77//PkcccUS9JgdIzH0Qq4BhkloRNDGdAswGtgEnExw5nAB806BRLZ0aPPc/qUE361xjU5tf+vHUv39/jjjiiIrprKwsnn/+eUpKSli7di0LFy5kwIAB33lPeno6Z555JgA/+MEP+Pe//73HetesWcOqVasYNmwYkigtLeWrr77i4IMP5qOPPmLChAkApKSk0K5dO6ZOncrFF19csZOOZWc9fPjw7zRbPfbYY7z55ptAcO/J0qVLWb16NSeddBL777//d9Z79dVXM2rUKG688UZeeOGFiqON+pSIcxAzJb0GfA6UAF8QNBmlA3+XdCuwA6j/2lZn6VTocjC069Ggm3XO1U3r1q0rXi9evJjHH3+cWbNm0aFDBy6//PKo9wG0aNGi4nVycjIlJSV7LPPyyy+zefPmipvM8vPzmTBhAvfeey+w5yWkZhb1stKUlBTKysoAKC0t/c62ImOfMmUKH3/8MTNmzCA9PZ1jjz2WoqKiKtfbt29fOnbsyLRp0/jiiy8YMWJE1M+nLhLS2G5m95jZwWY20Mx+Yma7zCzPzM42s8PMbLiZzWuwgIoLYdX/efOSc43ctm3baNu2Le3atWPdunW8//77tV5XVlYWU6ZMYcWKFaxYsYJZs2ZVNDOddNJJPPvss0Cw09+2bRunnnoqEyZMYMuWLQAVz3379mXOnDkAvP7665SWlkbdXn5+PhkZGaSnp7NgwQI+++wzAI455himTp3KypUrv7NeCI4iLrvsMi655JIqT87XhZ+NhSA5lBTBAd685FxjNnToUAYMGMDAgQO59tprOeaYY2q1nqVLl5Kbm0tm5rc3yR544IG0bNmSOXPm8NRTT/H+++9z2GGHkZmZyVdffcWgQYO44447OP744xkyZEjFCe3rrruOyZMnc+SRRzJ37lxatmwZdZtnn302BQUFDB48mPvvv5+jjjoKgG7duvHMM88wcuRIBg8ezGWXXVbxngsuuID8/HyuvPLKWtWzJjKrvwuBGlpmZqbNnj277iv64G6Y8SzcuRJatK55eeeamUWLFnHIIYckOgxXyYwZM7jrrruYNm1alctE++4kzTGzGruI2PcuvE2EpR9Bn2GeHJxzjcbvfvc7xo0bV3GyPB68iWnHBlg/369ecs41Kr/85S9ZuXIlw4cPj9s2PEEs+yh49hPUzjn3HZ4glk6F9AzoPjjRkTjn3D6leScI717DOeeq1Lz3ihsWwY5cb15yzrkomneC8O41nGsU6qO7b4AXXniB3NzcKufv3r2bjIwMfvWrX9VH2I2eJ4jO34f2vRIdiXOuGuXdfc+dO5frr7+eW2+9tWI6stuMmtSUIN577z0GDBjAyy+/XB9hVyla1x77ouZ7H0RxEaz8FH4wOtGRONe4TLoTcufX7zq7HwZnPlirt7700kv86U9/Yvfu3Rx99NE89dRTlJWVMWbMGObOnYuZMXbsWLp168bcuXO5+OKLSU9PZ9asWXskl6ysLP77v/+bxx57jM8++6yiE8CZM2dyyy23UFBQQFpaGtOmTaNFixbcfvvtTJ48maSkJK6//np++tOf0qtXL7Kzs+nQoQMzZszg7rvvZsqUKdx9991s3LiRZcuW0b17d+69916uvPJKduzYQVJSEk8//XTF3dMPPPAAWVlZJCUlcc4553DFFVfwk5/8hFmzZgHBzW+jR4+umI6X5psgVs+AkkLvXsO5Riw7O5vXX3+dTz/9lJSUFMaOHcuECRPo378/mzZtquhGOy8vjw4dOvDkk0/y1FNPMWTIkD3WtXPnTqZPn8748ePJzc0lKyuLI444gqKiIi655BImTpzI0KFDyc/Pp2XLljz99NOsXbuWefPmkZyc/J0+kqryxRdf8PHHH5OWlkZBQQGTJ08mLS2Nr776itGjRzNz5kzeeustJk2axKxZs0hPT2fLli1kZGSQlpZGdnY2AwcOZPz48YwZM6beP8/Kmm+CWDoVklKh77GJjsS5xqWWv/TjYcqUKXz22WcVfSYVFhbSu3dvTj/9dL7++mtuvvlmzjrrrJh6On3zzTc57bTTSEtLY9SoUWRmZvLII4+waNEi+vTpw9ChQwFo3759xbZvueUWkpOTgdi69x45ciRpaWkA7Nq1ixtvvJF58+aRkpLC0qVLK9Z71VVXkZ6e/p31Xn311YwfP56HHnqIV199lS+++GJvPqpaacYJYhr0Pgpatkl0JM65WjIzrrrqKn7zm9/sMe/LL79k0qRJPPHEE0ycOJFx48ZVu66srCxmzpxJ3759AdiwYQMff/wx7dq1i9rddizde1fuajyye+9HH32U3r1787e//Y3i4mLatGlT7XpHjRrFAw88wDHHHMPw4cPp0KFDtfWpD83zJPWOjZD7JfQ/MdGROOfq4NRTT+WVV15h06ZNQHC106pVq9i4cSNmxqhRoyqGIAVo27Yt27dv32M9W7duZebMmeTk5FR07/3EE0+QlZXFoYceysqVKyvWsW3bNkpLSxkxYgTPPPNMRffd0br3njhx4h7bKpefn89+++2HJF566SXKO04dMWIEzz//PIWFhd9Zb6tWrTj55JO58cYbG6R5CZprgvDuNZxrEg477DDuueceTj31VAYNGsSIESNYv349q1evruh2+9prr+WBBx4AYMyYMVxzzTV7XB47ceJETjvtNFJTUyvKzj//fF5//XWSkpLIysrihhtuYPDgwYwYMYJdu3Zx3XXX0b17dwYNGsTgwYN55ZVXALj33nv56U9/ynHHHVftFVY33ngjzz33HMOGDWPlypUV3YCfc845nHHGGWRmZjJkyBAee+yxivdcdtllpKamcsopp9Tr51iVhHT3HY4adw1gwHxgjJkVhfOeDKdrbPupdXffX74Ks/4MV70PScl7/37nmhnv7nvf8OCDD7Jr1y7uueeemN/TqLr7ltQTuAkYYGaFkl4BLgFelJQJxL9hbdCo4OGcc43Eueeey+rVq5k6dWqDbTNRJ6lTgHRJxUArYK2kZOD3wI+BCxIUl3PO7ZPeeuutBt9mg5+DMLM1wCPAKmAdkG9mHwA3Am+a2brq3i9prKTZkmZv3Lgx/gE75wBozKNPNld1/c4aPEFI6giMBPoBPYDWkq4ARgFP1vR+MxtnZplmltmlS5f4BuucAyAtLY3Nmzd7kmhEzIzNmzdX3HdRG4loYjoVWG5mGwEk/RO4D0gHloTX/7aStMTMvpeA+JxzlfTq1YucnBz8qL1xSUtLo1ev2vc1l4gEsQoYJqkVUAicAvzBzCqOHiTt8OTg3L4jNTWVfv36JToM18AScQ5iJvAa8DnBJa5JQPW3ODrnnGtwCbmKyczuAaq8kDeWeyCcc87FV/O8k9o551yNEnIndX2RtBFYWcNinYFNDRDOvsbr3bw013pD8617Xeq9v5nVeBloo04QsZA0O5Zbypsar3fz0lzrDc237g1Rb29ics45F5UnCOecc1E1hwTRXC+h9Xo3L8213tB86x73ejf5cxDOOedqpzkcQTjnnKsFTxDOOeeiatIJQtIZkr6WtETSnYmOp64kvSBpg6TsiLIMSZMlLQ6fO4blkvREWPcvJQ2NeM/ocPnFkkYnoi6xktRb0jRJiyQtkHRzWN6k6w0gKU3SLEnzwrrfF5b3kzQzrMfLklqE5S3D6SXh/L4R67orLP9a0umJqVHsJCVL+kLS2+F0k68zgKQVkuZLmitpdliWuL91M2uSDyAZWAocALQA5hGMYpfw2OpQp+OBoUB2RNnDwJ3h6zuBh8LXZwGTAAHDgJlheQawLHzuGL7umOi6VVPn/YCh4eu2wDfAgKZe7zBmAW3C16nAzLBOrwCXhOXPAjeEr38KPBu+vgR4OXw9IPz7b0nQzf5SIDnR9auh7v8N/AN4O5xu8nUO414BdK5UlrC/9aZ8BHEksMTMlpnZbmACwTgUjZaZfQxsqVQ8EngpfP0ScH5E+f9aYAbQQdJ+wOnAZDPbYmZbgcnAGfGPvnbMbJ2ZfR6+3g4sAnrSxOsNENZhRziZGj4MOJmgw0vYs+7ln8lrwCkK+s8fCUwws11mthxYQvD/sU+S1As4G3gunBZNvM41SNjfelNOED2B1RHTOWFZU9PNwlH4wueuYXlV9W+0n0vYfHA4wS/pZlHvsKllLrCB4B99KZBnZiXhIpH1qKhjOD8f6ETjq/sfgTuAsnC6E02/zuUM+EDSHEljw7KE/a0nakzqhqAoZc3pmt6q6t8oPxdJbYCJwC1mti34kRh90ShljbbeZlYKDJHUAXgdOCTaYuFzo6+7pHOADWY2R9KJ5cVRFm0yda7kGDNbK6krMFnSV9UsG/e6N+UjiBygd8R0L2BtgmKJp/XhYSXh84awvKr6N7rPRVIqQXL4u5n9Myxu8vWOZGZ5wEcEbc0dJJX/uIusR0Udw/ntCZokG1PdjwHOk7SCoFn4ZIIjiqZc5wpmtjZ83kDwg+BIEvi33pQTxGfAgeHVDy0ITmC9meCY4uFNoPwqhdHAGxHlV4RXOgwD8sPD0/eBEZI6hldDjAjL9klhe/LzwCIz+0PErCZdbwBJXcIjBySlEwzXuwiYBvwoXKxy3cs/kx8BUy04a/kmcEl4xU8/4EBgVsPUYu+Y2V1m1svM+hL8z041s8townUuJ6m1pLblrwn+RrNJ5N96os/ax/NBcJb/G4J2218mOp56qE8WsA4oJviVcDVBe+uHwOLwOSNcVsCfwrrPBzIj1nMVwUm7JcCYRNerhjofS3B4/CUwN3yc1dTrHcY7CPgirHs28Ouw/ACCnd0S4FWgZVieFk4vCecfELGuX4afydfAmYmuW4z1P5Fvr2Jq8nUO6zgvfCwo32cl8m/du9pwzjkXVVNuYnLOOVcHniCcc85F5QnCOedcVJ4gnHPOReUJwjnnXFSeIJxzzkXlCcK5vSRpiKSzIqbPUz11Jy/pFkmt6mNdztWV3wfh3F6SdCXBTUk3xmHdK8J1b9qL9yRb0GeTc/XKjyBckyWpr4KBhv6iYMCdD8IuK6It21/Se2Evmv+WdHBYPkpStoJBez4Ou225H7g4HNTlYklXSnoqXP5FSc8oGORomaQTFAz0tEjSixHbe0bSbH13IKCbgB7ANEnTwrJLFQwgky3poYj375B0v6SZwHBJD0paGA4c80h8PlHX7CT69nJ/+CNeD6AvUAIMCadfAS6vYtkPgQPD10cR9OkDQRcGPcPXHcLnK4GnIt5bMQ28SNDJXPmYBNuAwwh+jM2JiKW8u4Rkgk74BoXTKwgHjCFIFquALgQ9L08Fzg/nGXBR+boIupNQZJz+8EddH34E4Zq65WY2N3w9hyBpfEfYlfjRwKvh2At/JhjJDuAT4EVJ1xLszGPxlpkZQXJZb2bzzayMoH+d8u1fJOlzgr6WDiUYAa2yI4CPzGyjBWMd/J1gVEGAUoIebiFIQkXAc5IuBApijNO5ajXl8SCcA9gV8boUiNbElEQwIM2QyjPM7HpJRxGMcDZX0h7LVLPNskrbLwNSwt5FbwOOMLOtYdNTWpT1VDnoBVBk4XkHMyuRdCRwCkEPqDcSdJPtXJ34EYRr9sxsG7Bc0iioGAx+cPi6v5nNNLNfA5sI+tnfTjA+dm21A3YC+ZK6AWdGzItc90zgBEmdJSUDlwLTK68sPAJqb2bvArcAsSQx52rkRxDOBS4DnpF0N8HYzxMIul3+vaQDCX7NfxiWrQLuDJuj/mdvN2Rm8yR9QdDktIygGavcOGCSpHVmdpKkuwjGQhDwrpm9secaaQu8ISktXO7WvY3JuWj8MlfnnHNReROTc865qLyJyTUrkv5EMO5xpMfNbHwi4nFuX+ZNTM4556LyJibnnHNReYJwzjkXlScI55xzUXmCcM45F9X/ByxOFU2cO4lcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [100, 200, 500, 1000, 1500, 2000, 3000, 5000]\n",
    "y1 = [90.18, 94.23, 98.83, 99.98, 100.00, 100.00, 100.00, 100.00]\n",
    "y2 = [84.25, 86.07, 87.24, 87.60, 87.76, 87.87, 87.95, 87.95]\n",
    "\n",
    "plt.title(\"XGBoost Overfitting Resistence\")\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('test accuracy %')\n",
    "plt.plot(x, y1)\n",
    "plt.plot(x, y2)\n",
    "plt.legend(['Train Accuracy', 'Test Accuracy'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
